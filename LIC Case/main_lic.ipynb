{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_txt(pdf_file_path, txt_file_path):\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text().rstrip()\n",
    "\n",
    "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_to_txt('/Users/Desktop/ACM CONFERENCE WORK/FINAL_LIST_DOCS/L_I_C_Of_India_Anr_vs_Consumer_Education_Research_Centre_on_10_May_1995.PDF', 'lic.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the required NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_sentences(sentences, keyword):\n",
    "    related_sentences = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if keyword in sentence:\n",
    "            prev_sentence = sentences[i - 1].strip() if i > 0 else None\n",
    "            next_sentence = sentences[i + 1].strip() if i < len(sentences) - 1 else None\n",
    "            related_sentences.append({\n",
    "                \"Sentence\": sentence.strip(),\n",
    "                \"Previous\": prev_sentence,\n",
    "                \"Next\": next_sentence\n",
    "            })\n",
    "\n",
    "    return related_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Desktop/ACM CONFERENCE WORK/codes/LIC Case/lic.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize into sentences using nltk.sent_tokenize()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "keyword = \"contract\"\n",
    "result = []\n",
    "\n",
    "related_sentences = find_related_sentences(sentences, keyword)\n",
    "result.extend(related_sentences)\n",
    "\n",
    "# Create a DataFrame with columns \"Sentence,\" \"Previous,\" and \"Next\"\n",
    "output_df = pd.DataFrame(result, columns=[\"Sentence\", \"Previous\", \"Next\"])\n",
    "\n",
    "# Save results to a new CSV\n",
    "output_df.to_csv(\"output_sentences_lic.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read successfully with encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "encodings_to_try = ['utf-8', 'latin1', 'utf-16', 'ISO-8859-1']\n",
    "\n",
    "# Try reading the CSV file with different encodings\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        output_df = pd.read_csv(\"output_sentences_lic.csv\", encoding=encoding)\n",
    "        print(\"CSV file read successfully with encoding:\", encoding)\n",
    "        break  # Exit loop if successful\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Failed to read CSV file with encoding:\", encoding)\n",
    "        continue  # Try next encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Previous</th>\n",
       "      <th>Next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They cannot use\\nJudicial process to create ri...</td>\n",
       "      <td>As the proposals\\nof respondents 2 to 5 were r...</td>\n",
       "      <td>No rights would flow to any party to the propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The High Court was not justified in interferin...</td>\n",
       "      <td>Of India &amp; Anr vs Consumer Education &amp; Researc...</td>\n",
       "      <td>The actuarial principles are the calculations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since the policy\\nis commercial contract, the ...</td>\n",
       "      <td>In the event of the said conversion, there is ...</td>\n",
       "      <td>Shri Dhawan, learned Senior counsel for the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It has no power to impose any\\nunconstitutiona...</td>\n",
       "      <td>Of India &amp; Anr vs Consumer Education &amp; Researc...</td>\n",
       "      <td>The term insurance policy being\\ncheaper premi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of India &amp; Anr vs Consumer Education &amp; Researc...</td>\n",
       "      <td>In fact, this policy appears to be very\\npopul...</td>\n",
       "      <td>Convertible term insurance is\\ndesigned to mee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>He has either to accept or leave the services ...</td>\n",
       "      <td>In\\ndotted line contracts there would be no oc...</td>\n",
       "      <td>His option would be either to accept the unrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>With a view to have the services of the goods,...</td>\n",
       "      <td>His option would be either to accept the unrea...</td>\n",
       "      <td>In National Textiles Workers' Union etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The question then is whether a\\nclause in the ...</td>\n",
       "      <td>Of India &amp; Anr vs Consumer Education &amp; Researc...</td>\n",
       "      <td>It is settled law that the arms of the court\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>It is seen that the respondents are not seekin...</td>\n",
       "      <td>They are of public obligations to give prefere...</td>\n",
       "      <td>Their privilege and legitimate expectation to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>They are not seeking any mandamus to direct th...</td>\n",
       "      <td>We are of the considered view\\nthat they are r...</td>\n",
       "      <td>The rest of the conditions age etc are valid a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   They cannot use\\nJudicial process to create ri...   \n",
       "1   The High Court was not justified in interferin...   \n",
       "2   Since the policy\\nis commercial contract, the ...   \n",
       "3   It has no power to impose any\\nunconstitutiona...   \n",
       "4   Of India & Anr vs Consumer Education & Researc...   \n",
       "..                                                ...   \n",
       "75  He has either to accept or leave the services ...   \n",
       "76  With a view to have the services of the goods,...   \n",
       "77  The question then is whether a\\nclause in the ...   \n",
       "78  It is seen that the respondents are not seekin...   \n",
       "79  They are not seeking any mandamus to direct th...   \n",
       "\n",
       "                                             Previous  \\\n",
       "0   As the proposals\\nof respondents 2 to 5 were r...   \n",
       "1   Of India & Anr vs Consumer Education & Researc...   \n",
       "2   In the event of the said conversion, there is ...   \n",
       "3   Of India & Anr vs Consumer Education & Researc...   \n",
       "4   In fact, this policy appears to be very\\npopul...   \n",
       "..                                                ...   \n",
       "75  In\\ndotted line contracts there would be no oc...   \n",
       "76  His option would be either to accept the unrea...   \n",
       "77  Of India & Anr vs Consumer Education & Researc...   \n",
       "78  They are of public obligations to give prefere...   \n",
       "79  We are of the considered view\\nthat they are r...   \n",
       "\n",
       "                                                 Next  \n",
       "0   No rights would flow to any party to the propo...  \n",
       "1   The actuarial principles are the calculations ...  \n",
       "2   Shri Dhawan, learned Senior counsel for the re...  \n",
       "3   The term insurance policy being\\ncheaper premi...  \n",
       "4   Convertible term insurance is\\ndesigned to mee...  \n",
       "..                                                ...  \n",
       "75  His option would be either to accept the unrea...  \n",
       "76           In National Textiles Workers' Union etc.  \n",
       "77  It is settled law that the arms of the court\\n...  \n",
       "78  Their privilege and legitimate expectation to ...  \n",
       "79  The rest of the conditions age etc are valid a...  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sen = pd.DataFrame(output_df['Previous'].astype(str) + ' ' + output_df['Sentence'].astype(str) + ' ' + output_df['Next'].astype(str), columns=['concat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the proposals\n",
      "of respondents 2 to 5 were rejected as not being in conformity with the conditions prescribed in\n",
      "Table 58, they cannot enforce any right flowing from Table 58 under Article 226. They cannot use\n",
      "Judicial process to create rights in their favour unless a binding contract emerged by acceptance of\n",
      "the proposal of insurance and acted upon. No rights would flow to any party to the proposal to\n",
      "challenge the policy, its terms and plan of insurance.\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_sen['concat'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sen.to_csv('sentences_final_concatenated_lic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_id_map(file_path):\n",
    "    person_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            person_id_list = row[0]\n",
    "            person = person_id_list.strip()\n",
    "            person_id = row[1].strip()\n",
    "            person_id_map[person] = person_id\n",
    "\n",
    "    return person_id_map\n",
    "\n",
    "def create_location_id_map(file_path):\n",
    "    location_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            location_id_list = row[0]\n",
    "            location = location_id_list.strip()\n",
    "            location_id = row[1].strip()\n",
    "            location_id_map[location] = location_id\n",
    "\n",
    "    return location_id_map\n",
    "\n",
    "def create_time_id_map(file_path):\n",
    "    time_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            time_id_list = row[0].split(',')\n",
    "            time = time_id_list.strip()\n",
    "            time_id = row[1].strip()\n",
    "            time_id_map[time] = time_id\n",
    "\n",
    "    return time_id_map\n",
    "\n",
    "def create_event_id_map(file_path):\n",
    "    event_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            event_id_list = row[0].split(',')\n",
    "            event = event_id_list.strip()\n",
    "            event_id = row[1].strip()\n",
    "            event_id_map[event] = event_id\n",
    "\n",
    "    return event_id_map\n",
    "\n",
    "def create_other_id_map(file_path):\n",
    "    other_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            other_id_list = row[0].split(',')\n",
    "            other = other_id_list.strip()\n",
    "            other_id = row[1].strip()\n",
    "            other_id_map[other] = other_id\n",
    "\n",
    "    return other_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_person = '/Users/Desktop/ACM CONFERENCE WORK/Entities/LIC Case/person.csv'\n",
    "file_path_location = '/Users/Desktop/ACM CONFERENCE WORK/Entities/LIC Case/location.csv'\n",
    "file_path_time = '/Users/Desktop/ACM CONFERENCE WORK/Entities/LIC Case/time.csv'\n",
    "file_path_event = '/Users/Desktop/ACM CONFERENCE WORK/Entities/LIC Case/event.csv'\n",
    "file_path_other = '/Users/Desktop/ACM CONFERENCE WORK/Entities/LIC Case/activity.csv'\n",
    "person_id_map = create_person_id_map(file_path_person)\n",
    "location_id_map = create_location_id_map(file_path_location)\n",
    "time_id_map = create_location_id_map(file_path_time)\n",
    "event_id_map = create_location_id_map(file_path_event)\n",
    "other_id_map = create_location_id_map(file_path_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n=6):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for j in range(n, 0, -1):\n",
    "        for i in range(len(words)):\n",
    "            if i + j <= len(words):\n",
    "                ngrams.append(' '.join(words[i:i+j]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\";\", \"\")\n",
    "#     text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"!\", \"\")\n",
    "    text = text.replace(\"?\", \"\")\n",
    "#     text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entities(text):\n",
    "    text = text.lower()\n",
    "    ngrams = generate_ngrams(text)\n",
    "    replaced_text = text\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        original = ngram.lower()\n",
    "        \n",
    "        if original in person_id_map:\n",
    "            entity_id = person_id_map[original]\n",
    "            replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "        else:\n",
    "            preprocessed_ngram = preprocess_text(ngram.lower())\n",
    "            if preprocessed_ngram in person_id_map:\n",
    "                entity_id = person_id_map[preprocessed_ngram]\n",
    "                replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "            else:\n",
    "                if original in location_id_map:\n",
    "                    entity_id = location_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in location_id_map:\n",
    "                    entity_id = location_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in time_id_map:\n",
    "                    entity_id = time_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in time_id_map:\n",
    "                    entity_id = time_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in event_id_map:\n",
    "                    entity_id = event_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in event_id_map:\n",
    "                    entity_id = event_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in other_id_map:\n",
    "                    entity_id = other_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in other_id_map:\n",
    "                    entity_id = other_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                    \n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_final_concatenated_lic.csv', 'r', encoding='latin-1') as csvfile:\n",
    "     reader = csv.reader(csvfile)\n",
    "     next(reader)\n",
    "     ref_sen = []\n",
    "     for row in reader:\n",
    "         sentence = row[0]\n",
    "         ref_sen.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentences = []\n",
    "\n",
    "for sen in ref_sen:\n",
    "     s = replace_entities(sen)\n",
    "     final_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_sentences_lic.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Sentences'])\n",
    "#      for sen in final_sentences:\n",
    "    for sen in final_sentences:\n",
    "        writer.writerow([sen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the sentences\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Load the CSV files\n",
    "time_df = pd.read_csv(file_path_time)\n",
    "location_df = pd.read_csv(file_path_location)\n",
    "event_df = pd.read_csv(file_path_event)\n",
    "person_df = pd.read_csv(file_path_person)\n",
    "activity_df = pd.read_csv(file_path_other)\n",
    "\n",
    "location_ID = location_df['ID'].tolist()\n",
    "location_df = location_df['Location'].tolist()\n",
    "\n",
    "time_ID = time_df['ID'].tolist()\n",
    "time_df = time_df['Time'].tolist()\n",
    "\n",
    "# event_ID = event_df['ID'].tolist()\n",
    "# event_df = event_df['Event'].tolist()\n",
    "\n",
    "# person_ID = person_df['ID'].tolist()\n",
    "# person_df = person_df['Person'].tolist()\n",
    "\n",
    "# activity_ID = activity_df['ID'].tolist()\n",
    "# activity_df = activity_df['Others'].tolist()\n",
    "\n",
    "event_ID_map = dict(zip(event_df['ID'], event_df['Event']))\n",
    "person_ID_map = dict(zip(person_df['ID'], person_df['Person']))\n",
    "activity_ID_map = dict(zip(activity_df['ID'], activity_df['Activity']))\n",
    "\n",
    "matrix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('cleaned_sentences_lic.csv', 'r', encoding='latin-1') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sentence = row[0]\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    pattern = r'([PTLEA]\\d+)'\n",
    "    matches = re.findall(pattern, s)\n",
    "\n",
    "    persons = []\n",
    "    time = []\n",
    "    location = []\n",
    "    event = []\n",
    "    activity = []\n",
    "\n",
    "    for match in matches:\n",
    "        if match[0] == 'P':\n",
    "            persons.append(person_ID_map.get(match, 'Unknown Person'))\n",
    "        elif match[0] == 'L':\n",
    "             location.append(location_ID.index(match))\n",
    "        elif match[0] == 'E':\n",
    "            event.append(event_ID_map.get(match, 'Unknown Event'))\n",
    "        elif match[0] == 'A':\n",
    "            activity.append(activity_ID_map.get(match, 'Unknown Activity'))\n",
    "        elif match[0] == 'T':\n",
    "            time.append(time_ID.index(match))\n",
    "\n",
    "    if location and time:\n",
    "        for loc_index in location:\n",
    "            loc_key = location_df[loc_index]\n",
    "            for time_index in time:\n",
    "                time_key = time_df[time_index]\n",
    "                if time_key not in matrix:\n",
    "                    matrix[time_key] = {}\n",
    "                if loc_key not in matrix[time_key]:\n",
    "                    matrix[time_key][loc_key] = []\n",
    "                matrix[time_key][loc_key].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    elif location:\n",
    "        for loc_index in location:\n",
    "            loc_key = location_df[loc_index]\n",
    "            if loc_key not in matrix:\n",
    "                matrix[loc_key] = {}\n",
    "            if 'NULL' not in matrix:\n",
    "                matrix['NULL'] = {}\n",
    "            if loc_key not in matrix[\"NULL\"]:\n",
    "                matrix[\"NULL\"][loc_key] = []\n",
    "            matrix[\"NULL\"][loc_key].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    elif time:\n",
    "        for time_index in time:\n",
    "            time_key = time_df[time_index]\n",
    "            if time_key not in matrix:\n",
    "                matrix[time_key] = {}\n",
    "            if 'NULL' not in matrix[time_key]:\n",
    "                matrix[time_key]['NULL'] = []\n",
    "            matrix[time_key][\"NULL\"].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    else:\n",
    "        if 'NULL' not in matrix:\n",
    "            matrix['NULL'] = {}\n",
    "        if 'NULL' not in matrix['NULL']:\n",
    "            matrix['NULL']['NULL'] = []\n",
    "        matrix[\"NULL\"][\"NULL\"].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "\n",
    "matrix_df = pd.DataFrame.from_dict(matrix, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df.to_csv('matrix_with_sen_lic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('matrix_with_sen_lic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Unnamed: 0': 'Time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        # Convert the tuple to a string and add it as a node\n",
    "        G.add_node(f\"{time}, {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = set()\n",
    "\n",
    "# Update the final set with only the entities\n",
    "for value in pd.unique(df.drop(columns='Time').values.ravel()):\n",
    "    if pd.notnull(value):\n",
    "        value_list = ast.literal_eval(value)\n",
    "        entity_list = [entity for entity, sentence in value_list]\n",
    "        final.update(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        if pd.notnull(row[location]):\n",
    "            value_list = ast.literal_eval(row[location])\n",
    "            entity_list = [entity for entity, sentence in value_list]\n",
    "            for entity in entity_list:\n",
    "                if entity in final:\n",
    "                    # Convert the tuples to strings and add them as an edge\n",
    "                    G.add_edge(f\"{time}, {location}\", entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in list(G.nodes):\n",
    "    if not list(G.neighbors(node)):\n",
    "        G.remove_node(node)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node1 in G.nodes:\n",
    "    # Check if node1 can be split into a pair\n",
    "    if ', ' in node1:\n",
    "        parts1 = node1.split(\", \")\n",
    "        if len(parts1) == 2:  # Ensure it splits into exactly two parts\n",
    "            time1, location1 = parts1\n",
    "            for node2 in G.nodes:\n",
    "                # Check if node2 can be split into a pair\n",
    "                if ', ' in node2:\n",
    "                    parts2 = node2.split(\", \")\n",
    "                    if len(parts2) == 2:  # Ensure it splits into exactly two parts\n",
    "                        time2, location2 = parts2\n",
    "                        # Add edge if the time or location matches\n",
    "                        if time1 == time2 or location1 == location2:\n",
    "                            G.add_edge(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the sentences for each node\n",
    "sentences_dict = {}\n",
    "\n",
    "# Create a dictionary to store the sentences for each edge i.e. between two nodes\n",
    "edge_sentences_dict = {}\n",
    "\n",
    "# Update the sentences_dict and edge_sentences_dict\n",
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        if pd.notnull(row[location]):\n",
    "            value_list = ast.literal_eval(row[location])\n",
    "            for entity, sentence in value_list:\n",
    "                if entity in final:\n",
    "                    # Add the sentence to the sentences_dict for the entity\n",
    "                    if entity not in sentences_dict:\n",
    "                        sentences_dict[entity] = set()\n",
    "                    sentences_dict[entity].add(sentence)\n",
    "\n",
    "                    # Add the sentence to the sentences_dict for the (time, location) node\n",
    "                    time_location_node = f\"{time}, {location}\"\n",
    "                    if time_location_node not in sentences_dict:\n",
    "                        sentences_dict[time_location_node] = set()\n",
    "                    sentences_dict[time_location_node].add(sentence)\n",
    "\n",
    "                    # Add the sentence to the edge_sentences_dict for the edge\n",
    "                    edge = (time_location_node, entity)\n",
    "                    if edge not in edge_sentences_dict:\n",
    "                        edge_sentences_dict[edge] = set()\n",
    "                    edge_sentences_dict[edge].add(sentence)\n",
    "\n",
    "# Convert the sets back to lists if needed\n",
    "for node, sentences in sentences_dict.items():\n",
    "    sentences_dict[node] = list(sentences)\n",
    "\n",
    "for edge, sentences in edge_sentences_dict.items():\n",
    "    edge_sentences_dict[edge] = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: node2vec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (4.4.0)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (1.4.2)\n",
      "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.15.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e5d38440646a6be2090d5993a6a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:00<00:00, 1787.56it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:00<00:00, 2254.78it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:00<00:00, 2397.18it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:00<00:00, 2404.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "# Create a Node2Vec instance\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# Generate embeddings\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the node embeddings\n",
    "node_embeddings = {node: model.wv[node] for node in G.nodes}\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(list(node_embeddings.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=3)  # Adjust parameters as needed\n",
    "\n",
    "# Fit and predict clusters\n",
    "labels = dbscan.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\n",
    "    'rgb(31, 119, 180)',  # Blue\n",
    "    'rgb(255, 127, 14)',  # Orange\n",
    "    'rgb(44, 160, 44)',   # Green\n",
    "    'rgb(214, 39, 40)',   # Red\n",
    "    'rgb(148, 103, 189)', # Purple\n",
    "    'rgb(140, 86, 75)',   # Brown\n",
    "    'rgb(227, 119, 194)', # Pink\n",
    "    'rgb(127, 127, 127)', # Gray\n",
    "    'rgb(188, 189, 34)',  # Olive\n",
    "    'rgb(23, 190, 207)',  # Teal\n",
    "    'rgb(255, 187, 120)', # Peach\n",
    "    'rgb(214, 39, 40)',   # Maroon\n",
    "    'rgb(77, 175, 74)',   # Light Green\n",
    "    'rgb(152, 78, 163)',  # Plum\n",
    "    'rgb(255, 152, 150)'  # Salmon\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          -0.164962971160001,
          -0.46595259392305244,
          null,
          -0.164962971160001,
          -0.164962971160001,
          null,
          -0.164962971160001,
          0.16500567169623073,
          null,
          0.16500567169623073,
          0.4659098933868227,
          null,
          0.16500567169623073,
          0.16500567169623073,
          null
         ],
         "y": [
          0.35402992621723145,
          0.9999999999999999,
          null,
          0.35402992621723145,
          0.35402992621723145,
          null,
          0.35402992621723145,
          -0.3541273193130077,
          null,
          -0.3541273193130077,
          -0.9999026069042238,
          null,
          -0.3541273193130077,
          -0.3541273193130077,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(31, 119, 180)",
          "line": {
           "color": "black",
           "width": 2
          },
          "showscale": false,
          "size": 20
         },
         "mode": "markers+text",
         "text": [
          "nan, NULL",
          "1995.0, NULL",
          "declaring the offending portion",
          "terminating the tenancy"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          -0.164962971160001,
          0.16500567169623073,
          0.4659098933868227,
          -0.46595259392305244
         ],
         "y": [
          0.35402992621723145,
          -0.3541273193130077,
          -0.9999026069042238,
          0.9999999999999999
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hovermode": "closest",
        "margin": {
         "b": 20,
         "l": 5,
         "r": 5,
         "t": 40
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cluster 1"
        },
        "width": 600,
        "xaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        },
        "yaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Assuming 'G' is your graph and 'labels' is the list of labels\n",
    "# Renumber clusters from 1 to 7 if needed, otherwise ensure you have clusters numbered 1 to 7\n",
    "unique_labels = list(set(labels))\n",
    "label_mapping = {label: idx+1 for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "def draw_subgraph_plotly(subgraph, cluster_id, cluster_color):\n",
    "    pos = nx.spring_layout(subgraph, seed=42)  # Fixed seed for reproducibility\n",
    "    \n",
    "    # Extract the node positions\n",
    "    x_nodes = [pos[node][0] for node in subgraph.nodes()]\n",
    "    y_nodes = [pos[node][1] for node in subgraph.nodes()]\n",
    "\n",
    "    # Extract the edges\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in subgraph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    # Edge trace\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1, color='gray'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines')\n",
    "\n",
    "    # Node trace\n",
    "    node_trace = go.Scatter(\n",
    "        x=x_nodes, y=y_nodes,\n",
    "        mode='markers+text',\n",
    "        text=[f'{node}' for node in subgraph.nodes()],\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            color=cluster_color,  # Assign the cluster color\n",
    "            size=20,\n",
    "            line=dict(width=2, color='black')\n",
    "        ),\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=f'Cluster {cluster_id}',\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20, l=5, r=5, t=40),\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        height=600,\n",
    "                        width=600,\n",
    "                        paper_bgcolor='white',\n",
    "                        plot_bgcolor='white'\n",
    "                    ))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Iterate over each cluster and draw the subgraph\n",
    "for original_cluster_id in set(labels):\n",
    "    cluster_id = label_mapping[original_cluster_id]\n",
    "    cluster_color = cluster_colors[cluster_id - 1]  # Assign a unique color to each cluster\n",
    "    cluster_nodes = [node for node, label in zip(G.nodes, labels) if label == original_cluster_id]\n",
    "    subgraph = G.subgraph(cluster_nodes)\n",
    "    draw_subgraph_plotly(subgraph, cluster_id, cluster_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists for nodes and links\n",
    "# nodes = []\n",
    "# links = []\n",
    "\n",
    "# # Add nodes to the list\n",
    "# for node, sentences in sentences_dict.items():\n",
    "#     nodes.append({\n",
    "#         \"id\": node,\n",
    "#         \"group\": 1,  # Update this as needed\n",
    "#         \"size\": len(sentences)  # The size could be based on the number of sentences\n",
    "#     })\n",
    "\n",
    "# # Add links to the list\n",
    "# for edge, sentences in edge_sentences_dict.items():\n",
    "#     source, target = edge\n",
    "#     links.append({\n",
    "#         \"source\": source,\n",
    "#         \"target\": target,\n",
    "#         \"value\": len(sentences)  # The value could be based on the number of sentences\n",
    "#     })\n",
    "\n",
    "# # Combine nodes and links into a single dictionary\n",
    "# graph_data = {\n",
    "#     \"nodes\": nodes,\n",
    "#     \"links\": links\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['METIS_DLL'] = '/Users/.local/lib/libmetis.dylib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgecuts, parts = metis.part_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a mapping from nodes to integers\n",
    "node_to_int = {node: i for i, node in enumerate(G.nodes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/networkx/readwrite/json_graph/node_link.py:142: FutureWarning:\n",
      "\n",
      "\n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "\n",
    "data = nx.node_link_data(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = {\n",
    "    \"nodes\": [{\"name\": str(node), \"n\": degree_dict[node], \"grp\": int(labels[i]), \"id\": str(node)} for i, node in enumerate(G.nodes())],\n",
    "    \"links\": [{\"source\": str(link_data['source']), \"target\": str(link_data['target']), \"value\": 1} for link_data in data['links']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(graph_data)\n",
    "with open('data_lic.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Create a dictionary where keys are part numbers and values are lists of nodes\n",
    "# part_dict = defaultdict(list)\n",
    "# for node, part in enumerate(parts):\n",
    "#     part_dict[part].append(node)\n",
    "\n",
    "# # Print the number of parts\n",
    "# print(\"Number of parts:\", len(part_dict))\n",
    "\n",
    "# # Print the nodes in each part\n",
    "# for part, nodes in part_dict.items():\n",
    "#     print(\"Part\", part, \":\", nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already created clusters using DBSCAN and have 'labels' and 'G' available\n",
    "\n",
    "# Initialize an empty dictionary to store sentences for each cluster\n",
    "cluster_sentences_dict = {}\n",
    "\n",
    "# Iterate over each cluster\n",
    "for original_cluster_id in set(labels):\n",
    "    cluster_id = label_mapping[original_cluster_id]\n",
    "    cluster_nodes = [node for node, label in zip(G.nodes, labels) if label == original_cluster_id]\n",
    "    \n",
    "    # Initialize a set to store sentences for this cluster\n",
    "    cluster_sentences = set()\n",
    "    \n",
    "    # Iterate over nodes in the cluster\n",
    "    for node in cluster_nodes:\n",
    "        # Assuming 'node' contains the relevant information (e.g., entity, time, location)\n",
    "        # Extract sentences associated with this node and add them to the cluster_sentences set\n",
    "        if node in sentences_dict:\n",
    "            cluster_sentences.update(sentences_dict[node])\n",
    "    \n",
    "    # Store the cluster_sentences set in the cluster_sentences_dict\n",
    "    cluster_sentences_dict[cluster_id] = cluster_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'terminating the tenancy': ['in that case when the\\nbuilding owned by the port trust was exempted from the rent act, on A28 for\\ndevelopment when possession was sought to be taken, it was challenged under article 226 that the\\naction of the port trust was arbitrary and no public interest would be served by terminating the\\ntenancy. in that context, this court held that even in contractual relations the court cannot ignore\\nthat the public authority must have constitutional conscience so that any interpretation put up must\\nbe to avoid arbitrary action, lest the authority would be permitted to flourish as imperium a imperia. whatever be the activity of the public authority, it must meet the test of article 14 and judicial\\nreview strikes an arbitrary action.'],\n",
       " 'nan, NULL': ['in that case when the\\nbuilding owned by the port trust was exempted from the rent act, on A28 for\\ndevelopment when possession was sought to be taken, it was challenged under article 226 that the\\naction of the port trust was arbitrary and no public interest would be served by terminating the\\ntenancy. in that context, this court held that even in contractual relations the court cannot ignore\\nthat the public authority must have constitutional conscience so that any interpretation put up must\\nbe to avoid arbitrary action, lest the authority would be permitted to flourish as imperium a imperia. whatever be the activity of the public authority, it must meet the test of article 14 and judicial\\nreview strikes an arbitrary action.'],\n",
       " 'declaring the offending portion': ['of india & anr vs consumer education & research centre & ... on 10 may, T25\\nindian kanoon - http://indiankanoon.org/doc/1513693/ 2policy to suit the interests of all those interested in obtaining a particular policy and their viability. the high court was not justified in interfering with matters based on economic criteria and\\ncommercial contracts, in particular, after having recorded findings referred to hereinbefore in\\nfavour of the corporation, the high court committed error of law in E21\\nof the policy as arbitrary and violative of articles 14, 19 and 21 of the constitution. the actuarial principles are the calculations made by actuaries taking into consideration:\\na) present condition of health and physical build of the life to be insured;\\nb) personal and family history, occupation, likelihood of any change in the\\noccupation etc.'],\n",
       " '1995.0, NULL': ['of india & anr vs consumer education & research centre & ... on 10 may, T25\\nindian kanoon - http://indiankanoon.org/doc/1513693/ 2policy to suit the interests of all those interested in obtaining a particular policy and their viability. the high court was not justified in interfering with matters based on economic criteria and\\ncommercial contracts, in particular, after having recorded findings referred to hereinbefore in\\nfavour of the corporation, the high court committed error of law in E21\\nof the policy as arbitrary and violative of articles 14, 19 and 21 of the constitution. the actuarial principles are the calculations made by actuaries taking into consideration:\\na) present condition of health and physical build of the life to be insured;\\nb) personal and family history, occupation, likelihood of any change in the\\noccupation etc.']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(str(sentences_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
