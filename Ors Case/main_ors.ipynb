{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_txt(pdf_file_path, txt_file_path):\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text().rstrip()\n",
    "\n",
    "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_to_txt('/Users/ashnadua/Desktop/ACM CONFERENCE WORK/FINAL_LIST_DOCS/Selvi_Ors_vs_State_Of_Karnataka_Anr_on_5_May_2010.PDF', 'ors.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ashnadua/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the required NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_sentences(sentences, keyword):\n",
    "    related_sentences = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if keyword in sentence:\n",
    "            prev_sentence = sentences[i - 1].strip() if i > 0 else None\n",
    "            next_sentence = sentences[i + 1].strip() if i < len(sentences) - 1 else None\n",
    "            related_sentences.append({\n",
    "                \"Sentence\": sentence.strip(),\n",
    "                \"Previous\": prev_sentence,\n",
    "                \"Next\": next_sentence\n",
    "            })\n",
    "\n",
    "    return related_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/ashnadua/Desktop/ACM CONFERENCE WORK/codes/Ors Case/ors.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize into sentences using nltk.sent_tokenize()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "keyword = \"polygraph\"\n",
    "result = []\n",
    "\n",
    "related_sentences = find_related_sentences(sentences, keyword)\n",
    "result.extend(related_sentences)\n",
    "\n",
    "# Create a DataFrame with columns \"Sentence,\" \"Previous,\" and \"Next\"\n",
    "output_df = pd.DataFrame(result, columns=[\"Sentence\", \"Previous\", \"Next\"])\n",
    "\n",
    "# Save results to a new CSV\n",
    "output_df.to_csv(\"output_sentences_ors.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read successfully with encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "encodings_to_try = ['utf-8', 'latin1', 'utf-16', 'ISO-8859-1']\n",
    "\n",
    "# Try reading the CSV file with different encodings\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        output_df = pd.read_csv(\"output_sentences_ors.csv\", encoding=encoding)\n",
    "        print(\"CSV file read successfully with encoding:\", encoding)\n",
    "        break  # Exit loop if successful\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Failed to read CSV file with encoding:\", encoding)\n",
    "        continue  # Try next encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Previous</th>\n",
       "      <th>Next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The legal questions in this batch of criminal ...</td>\n",
       "      <td>1.</td>\n",
       "      <td>This issue has received considerable attention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In one of the impugned judgments, it has been ...</td>\n",
       "      <td>The involuntary administration of the impugned...</td>\n",
       "      <td>It was\\nfurther ruled that the verbal revelati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The origins of polygraph examination have been...</td>\n",
       "      <td>DESCRIPTIONS OF TESTS - USES, LIMITATIONS AND\\...</td>\n",
       "      <td>His device was called a hydrosphygmograph.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1921, John Larson\\nincorporated the measure...</td>\n",
       "      <td>A similar device was used by psychologist Will...</td>\n",
       "      <td>10.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The theory behind polygraph tests is that when...</td>\n",
       "      <td>10.</td>\n",
       "      <td>During the polygraph examination, several inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>In the case of polygraph examination and the B...</td>\n",
       "      <td>When a person undergoes a narcoanalysis test, ...</td>\n",
       "      <td>However, when he/she later learns about the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>In the context of\\npolygraph examination and t...</td>\n",
       "      <td>In case of the narcoanalysis technique, the su...</td>\n",
       "      <td>Furthermore, the results\\nare derived from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>As far as polygraph examination is concerned, ...</td>\n",
       "      <td>Furthermore, empirical studies have shown that...</td>\n",
       "      <td>Objections can be raised about the qualificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>A significant\\ncriticism of polygraphy is that...</td>\n",
       "      <td>Objections can be raised about the qualificati...</td>\n",
       "      <td>Similarly, with the P300 Waves\\ntest there are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>In a polygraph test, interpreting the results ...</td>\n",
       "      <td>Sometimes the revelations may\\nbegin to make s...</td>\n",
       "      <td>In a BEAP test, there is always the possibilit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    The legal questions in this batch of criminal ...   \n",
       "1    In one of the impugned judgments, it has been ...   \n",
       "2    The origins of polygraph examination have been...   \n",
       "3    In 1921, John Larson\\nincorporated the measure...   \n",
       "4    The theory behind polygraph tests is that when...   \n",
       "..                                                 ...   \n",
       "129  In the case of polygraph examination and the B...   \n",
       "130  In the context of\\npolygraph examination and t...   \n",
       "131  As far as polygraph examination is concerned, ...   \n",
       "132  A significant\\ncriticism of polygraphy is that...   \n",
       "133  In a polygraph test, interpreting the results ...   \n",
       "\n",
       "                                              Previous  \\\n",
       "0                                                   1.   \n",
       "1    The involuntary administration of the impugned...   \n",
       "2    DESCRIPTIONS OF TESTS - USES, LIMITATIONS AND\\...   \n",
       "3    A similar device was used by psychologist Will...   \n",
       "4                                                  10.   \n",
       "..                                                 ...   \n",
       "129  When a person undergoes a narcoanalysis test, ...   \n",
       "130  In case of the narcoanalysis technique, the su...   \n",
       "131  Furthermore, empirical studies have shown that...   \n",
       "132  Objections can be raised about the qualificati...   \n",
       "133  Sometimes the revelations may\\nbegin to make s...   \n",
       "\n",
       "                                                  Next  \n",
       "0    This issue has received considerable attention...  \n",
       "1    It was\\nfurther ruled that the verbal revelati...  \n",
       "2           His device was called a hydrosphygmograph.  \n",
       "3                                                  10.  \n",
       "4    During the polygraph examination, several inst...  \n",
       "..                                                 ...  \n",
       "129  However, when he/she later learns about the co...  \n",
       "130  Furthermore, the results\\nare derived from the...  \n",
       "131  Objections can be raised about the qualificati...  \n",
       "132  Similarly, with the P300 Waves\\ntest there are...  \n",
       "133  In a BEAP test, there is always the possibilit...  \n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sen = pd.DataFrame(output_df['Previous'].astype(str) + ' ' + output_df['Sentence'].astype(str) + ' ' + output_df['Next'].astype(str), columns=['concat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The legal questions in this batch of criminal appeals relate to the involuntary administration ofSelvi & Ors vs State Of Karnataka & Anr on 5 May, 2010\n",
      "Indian Kanoon - http://indiankanoon.org/doc/338008/ 1certain scientific techniques, namely narcoanalysis, polygraph examination and the Brain Electrical\n",
      "Activation Profile (BEAP) test for the purpose of improving investigation efforts in criminal cases. This issue has received considerable attention since it involves tensions between the desirability of\n",
      "efficient investigation and the preservation of individual liberties.\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_sen['concat'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sen.to_csv('sentences_final_concatenated_ors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_id_map(file_path):\n",
    "    person_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            person_id_list = row[0]\n",
    "            person = person_id_list.strip()\n",
    "            person_id = row[1].strip()\n",
    "            person_id_map[person] = person_id\n",
    "\n",
    "    return person_id_map\n",
    "\n",
    "def create_location_id_map(file_path):\n",
    "    location_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            location_id_list = row[0]\n",
    "            location = location_id_list.strip()\n",
    "            location_id = row[1].strip()\n",
    "            location_id_map[location] = location_id\n",
    "\n",
    "    return location_id_map\n",
    "\n",
    "def create_time_id_map(file_path):\n",
    "    time_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            time_id_list = row[0].split(',')\n",
    "            time = time_id_list.strip()\n",
    "            time_id = row[1].strip()\n",
    "            time_id_map[time] = time_id\n",
    "\n",
    "    return time_id_map\n",
    "\n",
    "def create_event_id_map(file_path):\n",
    "    event_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            event_id_list = row[0].split(',')\n",
    "            event = event_id_list.strip()\n",
    "            event_id = row[1].strip()\n",
    "            event_id_map[event] = event_id\n",
    "\n",
    "    return event_id_map\n",
    "\n",
    "def create_other_id_map(file_path):\n",
    "    other_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            other_id_list = row[0].split(',')\n",
    "            other = other_id_list.strip()\n",
    "            other_id = row[1].strip()\n",
    "            other_id_map[other] = other_id\n",
    "\n",
    "    return other_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_person = '/Users/ashnadua/Desktop/ACM CONFERENCE WORK/Entities/Ors Case/person.csv'\n",
    "file_path_location = '/Users/ashnadua/Desktop/ACM CONFERENCE WORK/Entities/Ors Case/location.csv'\n",
    "file_path_time = '/Users/ashnadua/Desktop/ACM CONFERENCE WORK/Entities/Ors Case/time.csv'\n",
    "file_path_event = '/Users/ashnadua/Desktop/ACM CONFERENCE WORK/Entities/Ors Case/event.csv'\n",
    "file_path_other = '/Users/ashnadua/Desktop/ACM CONFERENCE WORK/Entities/Ors Case/activity.csv'\n",
    "person_id_map = create_person_id_map(file_path_person)\n",
    "location_id_map = create_location_id_map(file_path_location)\n",
    "time_id_map = create_location_id_map(file_path_time)\n",
    "event_id_map = create_location_id_map(file_path_event)\n",
    "other_id_map = create_location_id_map(file_path_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n=6):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for j in range(n, 0, -1):\n",
    "        for i in range(len(words)):\n",
    "            if i + j <= len(words):\n",
    "                ngrams.append(' '.join(words[i:i+j]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\";\", \"\")\n",
    "#     text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"!\", \"\")\n",
    "    text = text.replace(\"?\", \"\")\n",
    "#     text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entities(text):\n",
    "    text = text.lower()\n",
    "    ngrams = generate_ngrams(text)\n",
    "    replaced_text = text\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        original = ngram.lower()\n",
    "        \n",
    "        if original in person_id_map:\n",
    "            entity_id = person_id_map[original]\n",
    "            replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "        else:\n",
    "            preprocessed_ngram = preprocess_text(ngram.lower())\n",
    "            if preprocessed_ngram in person_id_map:\n",
    "                entity_id = person_id_map[preprocessed_ngram]\n",
    "                replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "            else:\n",
    "                if original in location_id_map:\n",
    "                    entity_id = location_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in location_id_map:\n",
    "                    entity_id = location_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in time_id_map:\n",
    "                    entity_id = time_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in time_id_map:\n",
    "                    entity_id = time_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in event_id_map:\n",
    "                    entity_id = event_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in event_id_map:\n",
    "                    entity_id = event_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in other_id_map:\n",
    "                    entity_id = other_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in other_id_map:\n",
    "                    entity_id = other_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                    \n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_final_concatenated_ors.csv', 'r', encoding='latin-1') as csvfile:\n",
    "     reader = csv.reader(csvfile)\n",
    "     next(reader)\n",
    "     ref_sen = []\n",
    "     for row in reader:\n",
    "         sentence = row[0]\n",
    "         ref_sen.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentences = []\n",
    "\n",
    "for sen in ref_sen:\n",
    "     s = replace_entities(sen)\n",
    "     final_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_sentences_ors.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Sentences'])\n",
    "#      for sen in final_sentences:\n",
    "    for sen in final_sentences:\n",
    "        writer.writerow([sen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the sentences\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Load the CSV files\n",
    "time_df = pd.read_csv(file_path_time)\n",
    "location_df = pd.read_csv(file_path_location)\n",
    "event_df = pd.read_csv(file_path_event)\n",
    "person_df = pd.read_csv(file_path_person)\n",
    "activity_df = pd.read_csv(file_path_other)\n",
    "\n",
    "location_ID = location_df['Id'].tolist()\n",
    "location_df = location_df['Location'].tolist()\n",
    "\n",
    "time_ID = time_df['Id'].tolist()\n",
    "time_df = time_df['Time'].tolist()\n",
    "\n",
    "# event_ID = event_df['ID'].tolist()\n",
    "# event_df = event_df['Event'].tolist()\n",
    "\n",
    "# person_ID = person_df['ID'].tolist()\n",
    "# person_df = person_df['Person'].tolist()\n",
    "\n",
    "# activity_ID = activity_df['ID'].tolist()\n",
    "# activity_df = activity_df['Others'].tolist()\n",
    "\n",
    "event_ID_map = dict(zip(event_df['Id'], event_df['Event']))\n",
    "person_ID_map = dict(zip(person_df['Id'], person_df['Person']))\n",
    "activity_ID_map = dict(zip(activity_df['Id'], activity_df['Activity']))\n",
    "\n",
    "matrix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('cleaned_sentences_ors.csv', 'r', encoding='latin-1') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sentence = row[0]\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    pattern = r'([PTLEA]\\d+)'\n",
    "    matches = re.findall(pattern, s)\n",
    "\n",
    "    persons = []\n",
    "    time = []\n",
    "    location = []\n",
    "    event = []\n",
    "    activity = []\n",
    "\n",
    "    for match in matches:\n",
    "        if match[0] == 'P':\n",
    "            persons.append(person_ID_map.get(match, 'Unknown Person'))\n",
    "        elif match[0] == 'L':\n",
    "             location.append(location_ID.index(match))\n",
    "        elif match[0] == 'E':\n",
    "            event.append(event_ID_map.get(match, 'Unknown Event'))\n",
    "        elif match[0] == 'A':\n",
    "            activity.append(activity_ID_map.get(match, 'Unknown Activity'))\n",
    "        elif match[0] == 'T':\n",
    "            time.append(time_ID.index(match))\n",
    "\n",
    "    if location and time:\n",
    "        for loc_index in location:\n",
    "            loc_key = location_df[loc_index]\n",
    "            for time_index in time:\n",
    "                time_key = time_df[time_index]\n",
    "                if time_key not in matrix:\n",
    "                    matrix[time_key] = {}\n",
    "                if loc_key not in matrix[time_key]:\n",
    "                    matrix[time_key][loc_key] = []\n",
    "                matrix[time_key][loc_key].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    elif location:\n",
    "        for loc_index in location:\n",
    "            loc_key = location_df[loc_index]\n",
    "            if loc_key not in matrix:\n",
    "                matrix[loc_key] = {}\n",
    "            if 'NULL' not in matrix:\n",
    "                matrix['NULL'] = {}\n",
    "            if loc_key not in matrix[\"NULL\"]:\n",
    "                matrix[\"NULL\"][loc_key] = []\n",
    "            matrix[\"NULL\"][loc_key].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    elif time:\n",
    "        for time_index in time:\n",
    "            time_key = time_df[time_index]\n",
    "            if time_key not in matrix:\n",
    "                matrix[time_key] = {}\n",
    "            if 'NULL' not in matrix[time_key]:\n",
    "                matrix[time_key]['NULL'] = []\n",
    "            matrix[time_key][\"NULL\"].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    else:\n",
    "        if 'NULL' not in matrix:\n",
    "            matrix['NULL'] = {}\n",
    "        if 'NULL' not in matrix['NULL']:\n",
    "            matrix['NULL']['NULL'] = []\n",
    "        matrix[\"NULL\"][\"NULL\"].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "\n",
    "matrix_df = pd.DataFrame.from_dict(matrix, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df.to_csv('matrix_with_sen_ors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('matrix_with_sen_ors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Unnamed: 0': 'Time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        # Convert the tuple to a string and add it as a node\n",
    "        G.add_node(f\"{time}, {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = set()\n",
    "\n",
    "# Update the final set with only the entities\n",
    "for value in pd.unique(df.drop(columns='Time').values.ravel()):\n",
    "    if pd.notnull(value):\n",
    "        value_list = ast.literal_eval(value)\n",
    "        entity_list = [entity for entity, sentence in value_list]\n",
    "        final.update(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        if pd.notnull(row[location]):\n",
    "            value_list = ast.literal_eval(row[location])\n",
    "            entity_list = [entity for entity, sentence in value_list]\n",
    "            for entity in entity_list:\n",
    "                if entity in final:\n",
    "                    # Convert the tuples to strings and add them as an edge\n",
    "                    G.add_edge(f\"{time}, {location}\", entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in list(G.nodes):\n",
    "    if not list(G.neighbors(node)):\n",
    "        G.remove_node(node)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node1 in G.nodes:\n",
    "    # Check if node1 can be split into a pair\n",
    "    if ', ' in node1:\n",
    "        parts1 = node1.split(\", \")\n",
    "        if len(parts1) == 2:  # Ensure it splits into exactly two parts\n",
    "            time1, location1 = parts1\n",
    "            for node2 in G.nodes:\n",
    "                # Check if node2 can be split into a pair\n",
    "                if ', ' in node2:\n",
    "                    parts2 = node2.split(\", \")\n",
    "                    if len(parts2) == 2:  # Ensure it splits into exactly two parts\n",
    "                        time2, location2 = parts2\n",
    "                        # Add edge if the time or location matches\n",
    "                        if time1 == time2 or location1 == location2:\n",
    "                            G.add_edge(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the sentences for each node\n",
    "sentences_dict = {}\n",
    "\n",
    "# Create a dictionary to store the sentences for each edge i.e. between two nodes\n",
    "edge_sentences_dict = {}\n",
    "\n",
    "# Update the sentences_dict and edge_sentences_dict\n",
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        if pd.notnull(row[location]):\n",
    "            value_list = ast.literal_eval(row[location])\n",
    "            for entity, sentence in value_list:\n",
    "                if entity in final:\n",
    "                    # Add the sentence to the sentences_dict for the entity\n",
    "                    if entity not in sentences_dict:\n",
    "                        sentences_dict[entity] = set()\n",
    "                    sentences_dict[entity].add(sentence)\n",
    "\n",
    "                    # Add the sentence to the sentences_dict for the (time, location) node\n",
    "                    time_location_node = f\"{time}, {location}\"\n",
    "                    if time_location_node not in sentences_dict:\n",
    "                        sentences_dict[time_location_node] = set()\n",
    "                    sentences_dict[time_location_node].add(sentence)\n",
    "\n",
    "                    # Add the sentence to the edge_sentences_dict for the edge\n",
    "                    edge = (time_location_node, entity)\n",
    "                    if edge not in edge_sentences_dict:\n",
    "                        edge_sentences_dict[edge] = set()\n",
    "                    edge_sentences_dict[edge].add(sentence)\n",
    "\n",
    "# Convert the sets back to lists if needed\n",
    "for node, sentences in sentences_dict.items():\n",
    "    sentences_dict[node] = list(sentences)\n",
    "\n",
    "for edge, sentences in edge_sentences_dict.items():\n",
    "    edge_sentences_dict[edge] = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: node2vec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (4.4.0)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (1.4.2)\n",
      "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.15.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565f526ef523434689d22553e0002a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:00<00:00, 1834.33it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:00<00:00, 1727.64it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:00<00:00, 1537.47it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:00<00:00, 1480.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "# Create a Node2Vec instance\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# Generate embeddings\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the node embeddings\n",
    "node_embeddings = {node: model.wv[node] for node in G.nodes}\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(list(node_embeddings.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=3)  # Adjust parameters as needed\n",
    "\n",
    "# Fit and predict clusters\n",
    "labels = dbscan.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\n",
    "    'rgb(31, 119, 180)',  # Blue\n",
    "    'rgb(255, 127, 14)',  # Orange\n",
    "    'rgb(44, 160, 44)',   # Green\n",
    "    'rgb(214, 39, 40)',   # Red\n",
    "    'rgb(148, 103, 189)', # Purple\n",
    "    'rgb(140, 86, 75)',   # Brown\n",
    "    'rgb(227, 119, 194)', # Pink\n",
    "    'rgb(127, 127, 127)', # Gray\n",
    "    'rgb(188, 189, 34)',  # Olive\n",
    "    'rgb(23, 190, 207)',  # Teal\n",
    "    'rgb(255, 187, 120)', # Peach\n",
    "    'rgb(214, 39, 40)',   # Maroon\n",
    "    'rgb(77, 175, 74)',   # Light Green\n",
    "    'rgb(152, 78, 163)',  # Plum\n",
    "    'rgb(255, 152, 150)'  # Salmon\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          -0.002686810144346607,
          -0.9999999999999999,
          null,
          -0.002686810144346607,
          0.7201383245361959,
          null,
          -0.002686810144346607,
          -0.4596474975211292,
          null,
          -0.002686810144346607,
          0.9045659754671256,
          null,
          -0.002686810144346607,
          -0.1623699923378461,
          null,
          -0.002686810144346607,
          -0.002686810144346607,
          null
         ],
         "y": [
          -0.0001550045502287649,
          -0.1563189047650767,
          null,
          -0.0001550045502287649,
          0.7155008815557055,
          null,
          -0.0001550045502287649,
          0.9042337190454924,
          null,
          -0.0001550045502287649,
          -0.46447997403480673,
          null,
          -0.0001550045502287649,
          -0.9987807172510865,
          null,
          -0.0001550045502287649,
          -0.0001550045502287649,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(31, 119, 180)",
          "line": {
           "color": "black",
           "width": 2
          },
          "showscale": false,
          "size": 20
         },
         "mode": "markers+text",
         "text": [
          "nan, NULL",
          "seizure of the drugs",
          "narcoanalysis interview",
          "polygraph examination",
          "investigation efforts",
          "investigation"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          -0.002686810144346607,
          0.9045659754671256,
          -0.1623699923378461,
          -0.9999999999999999,
          0.7201383245361959,
          -0.4596474975211292
         ],
         "y": [
          -0.0001550045502287649,
          -0.46447997403480673,
          -0.9987807172510865,
          -0.1563189047650767,
          0.7155008815557055,
          0.9042337190454924
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hovermode": "closest",
        "margin": {
         "b": 20,
         "l": 5,
         "r": 5,
         "t": 40
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cluster 1"
        },
        "width": 600,
        "xaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        },
        "yaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Assuming 'G' is your graph and 'labels' is the list of labels\n",
    "# Renumber clusters from 1 to 7 if needed, otherwise ensure you have clusters numbered 1 to 7\n",
    "unique_labels = list(set(labels))\n",
    "label_mapping = {label: idx+1 for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "def draw_subgraph_plotly(subgraph, cluster_id, cluster_color):\n",
    "    pos = nx.spring_layout(subgraph, seed=42)  # Fixed seed for reproducibility\n",
    "    \n",
    "    # Extract the node positions\n",
    "    x_nodes = [pos[node][0] for node in subgraph.nodes()]\n",
    "    y_nodes = [pos[node][1] for node in subgraph.nodes()]\n",
    "\n",
    "    # Extract the edges\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in subgraph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    # Edge trace\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1, color='gray'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines')\n",
    "\n",
    "    # Node trace\n",
    "    node_trace = go.Scatter(\n",
    "        x=x_nodes, y=y_nodes,\n",
    "        mode='markers+text',\n",
    "        text=[f'{node}' for node in subgraph.nodes()],\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            color=cluster_color,  # Assign the cluster color\n",
    "            size=20,\n",
    "            line=dict(width=2, color='black')\n",
    "        ),\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=f'Cluster {cluster_id}',\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20, l=5, r=5, t=40),\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        height=600,\n",
    "                        width=600,\n",
    "                        paper_bgcolor='white',\n",
    "                        plot_bgcolor='white'\n",
    "                    ))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Iterate over each cluster and draw the subgraph\n",
    "for original_cluster_id in set(labels):\n",
    "    cluster_id = label_mapping[original_cluster_id]\n",
    "    cluster_color = cluster_colors[cluster_id - 1]  # Assign a unique color to each cluster\n",
    "    cluster_nodes = [node for node, label in zip(G.nodes, labels) if label == original_cluster_id]\n",
    "    subgraph = G.subgraph(cluster_nodes)\n",
    "    draw_subgraph_plotly(subgraph, cluster_id, cluster_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists for nodes and links\n",
    "# nodes = []\n",
    "# links = []\n",
    "\n",
    "# # Add nodes to the list\n",
    "# for node, sentences in sentences_dict.items():\n",
    "#     nodes.append({\n",
    "#         \"id\": node,\n",
    "#         \"group\": 1,  # Update this as needed\n",
    "#         \"size\": len(sentences)  # The size could be based on the number of sentences\n",
    "#     })\n",
    "\n",
    "# # Add links to the list\n",
    "# for edge, sentences in edge_sentences_dict.items():\n",
    "#     source, target = edge\n",
    "#     links.append({\n",
    "#         \"source\": source,\n",
    "#         \"target\": target,\n",
    "#         \"value\": len(sentences)  # The value could be based on the number of sentences\n",
    "#     })\n",
    "\n",
    "# # Combine nodes and links into a single dictionary\n",
    "# graph_data = {\n",
    "#     \"nodes\": nodes,\n",
    "#     \"links\": links\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['METIS_DLL'] = '/Users/ashnadua/.local/lib/libmetis.dylib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgecuts, parts = metis.part_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a mapping from nodes to integers\n",
    "node_to_int = {node: i for i, node in enumerate(G.nodes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/networkx/readwrite/json_graph/node_link.py:142: FutureWarning:\n",
      "\n",
      "\n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "\n",
    "data = nx.node_link_data(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = {\n",
    "    \"nodes\": [{\"name\": str(node), \"n\": degree_dict[node], \"grp\": int(labels[i]), \"id\": str(node)} for i, node in enumerate(G.nodes())],\n",
    "    \"links\": [{\"source\": str(link_data['source']), \"target\": str(link_data['target']), \"value\": 1} for link_data in data['links']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(graph_data)\n",
    "with open('data_ors.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Create a dictionary where keys are part numbers and values are lists of nodes\n",
    "# part_dict = defaultdict(list)\n",
    "# for node, part in enumerate(parts):\n",
    "#     part_dict[part].append(node)\n",
    "\n",
    "# # Print the number of parts\n",
    "# print(\"Number of parts:\", len(part_dict))\n",
    "\n",
    "# # Print the nodes in each part\n",
    "# for part, nodes in part_dict.items():\n",
    "#     print(\"Part\", part, \":\", nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already created clusters using DBSCAN and have 'labels' and 'G' available\n",
    "\n",
    "# Initialize an empty dictionary to store sentences for each cluster\n",
    "cluster_sentences_dict = {}\n",
    "\n",
    "# Iterate over each cluster\n",
    "for original_cluster_id in set(labels):\n",
    "    cluster_id = label_mapping[original_cluster_id]\n",
    "    cluster_nodes = [node for node, label in zip(G.nodes, labels) if label == original_cluster_id]\n",
    "    \n",
    "    # Initialize a set to store sentences for this cluster\n",
    "    cluster_sentences = set()\n",
    "    \n",
    "    # Iterate over nodes in the cluster\n",
    "    for node in cluster_nodes:\n",
    "        # Assuming 'node' contains the relevant information (e.g., entity, time, location)\n",
    "        # Extract sentences associated with this node and add them to the cluster_sentences set\n",
    "        if node in sentences_dict:\n",
    "            cluster_sentences.update(sentences_dict[node])\n",
    "    \n",
    "    # Store the cluster_sentences set in the cluster_sentences_dict\n",
    "    cluster_sentences_dict[cluster_id] = cluster_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polygraph examination': ['however, this uncertainty was laid to rest by an authoritative ruling of the u.s. supreme\\ncourt in united states v. scheffer, 523 us 303 (1998). in that case, an eight judge majority decided\\nthat military rule of evidence 707 (which made polygraph results inadmissible in court-martial\\nproceedings) did not violate an accused person\\'s sixth amendment right to present a defence. the\\nrelevant part of the provision follows:\\n\"(a) notwithstanding any other provision of law, the results of a polygraph\\nexamination, the opinion of a polygraph examiner, or any reference to an offer to\\ntake, failure to take, or taking of a A3 shall not be admitted into\\nevidence.\"',\n",
       "  \"instead,\\ninferences are drawn from the measurement of physiological responses recorded during the\\nperformance of these tests. it could also be argued that tests such as A3 and the\\nbeap test do not involve a `positive volitional act' on part of the test subject and hence their results\\nshould not be treated as testimony. however, this does not entail that the results of these two testsselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 69should be likened to physical evidence and thereby excluded from the protective scope of article\\n20(3).\",\n",
       "  'as per the laboratory procedure manuals, the impugned tests are being conducted at the\\ndirection of jurisdictional courts even without obtaining the consent of the intended test subjects. in\\nmost cases these tests are conducted conjunctively wherein the veracity of the information revealed\\nthrough narcoanalysis is subsequently tested through a A3 or the beap test. in\\nsome cases the investigators could first want to ascertain the capacity of the subject to deceive\\n(through A3) or his/her familiarity with the relevant facts (through beap test)\\nbefore conducting a A2',\n",
       "  '...\"\\n[the following article includes a table which lists out the statutorily permissible uses of polygraph\\nexamination in the different state jurisdictions of the united states of america:\\nhenry t. greely and judy illes, `neuroscience based lie-\\ndetection: the urgent need for regulation\\', 33 american journal of law and\\nmedicine, 377-421 (2007)]\\n15. the propriety of compelling the victims of sexual offences to undergo a A3\\ncertainly merits consideration in the present case. it must also be noted that in some jurisdictions\\npolygraph tests have been permitted for the purpose of screening public employees, both at the\\nstage of recruitment and at regular intervals during the service-period.',\n",
       "  'the propriety of compelling the victims of sexual offences to undergo a A3\\ncertainly merits consideration in the present case. it must also be noted that in some jurisdictions\\npolygraph tests have been permitted for the purpose of screening public employees, both at the\\nstage of recruitment and at regular intervals during the service-period. in the u.s.a., the widespread\\nacceptance of polygraph tests for checking the antecedents and monitoring the conduct of public\\nemployees has encouraged private employers to resort to the same.',\n",
       "  'in this regard, para 3.4 (v) of the said manual reads as\\nfollows:\\n\"(v) in cases of alleged sex offences such as intercourse with a female child, forcible\\nrape, indecent liberties or perversion, it is important that the victim, as well as theselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 5accused, be made available for interview and A3 it is essential\\nthat the polygraph examiner get a first hand detailed statement from the victim, and\\nthe interview of the victim precede that of the suspect or witnesses. ...\"\\n[the following article includes a table which lists out the statutorily permissible uses of polygraph\\nexamination in the different state jurisdictions of the united states of america:\\nhenry t. greely and judy illes, `neuroscience based lie-\\ndetection: the urgent need for regulation\\', 33 american journal of law and\\nmedicine, 377-421 (2007)]\\n15.',\n",
       "  '10. the theory behind polygraph tests is that when a subject is lying in response to a question,\\nhe/she will produce physiological responses that are different from those that arise in the normal\\ncourse. during the A3 several instruments are attached to the subject for\\nmeasuring and recording the physiological responses.',\n",
       "  '14. another controversial use of polygraph tests has been on victims of sexual offences for testing\\nthe veracity of their allegations. while several states in the u.s.a. have enacted provisions to\\nprohibit such use, the text of the laboratory procedure manual for A3 [supra.]',\n",
       "  'descriptions of tests - uses, limitations and\\nprecedents A3\\n9. the origins of A3 have been traced back to the efforts of lombroso, a\\ncriminologist who experimented with a machine that measured blood pressure and pulse to assess\\nthe honesty of persons suspected of criminal conduct. his device was called a hydrosphygmograph.',\n",
       "  \"hence, on an aggregate understanding of the materials produced before us we lean towards the view\\nthat the impugned tests, i.e. the narcoanalysis technique, A3 and the beap test\\nshould not be read into the provisions for `medical examination' under the code of criminal\\nprocedure, 1973.selvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 67153. however, it must be borne in mind that even though the impugned techniques have not been\\nexpressly enumerated in the crpc, there is no statutory prohibition against them either.\",\n",
       "  'in united states v.\\ngalbreth, 908 f. supp 877 (d.n.m. 1995), the district court took note of new mexico rule of\\nevidence 11-707 which established standards for the admission of polygraph evidence. the said\\nprovision laid down that polygraph evidence would be admissible only when the following\\nconditions are met: the examiner must have had at least 5 years experience in conducting polygraph\\ntests and 20 hours of continuing education within the past year; the A3 must be\\ntape recorded in its entirety; the polygraph charts must be scored quantitatively in a manner\\ngenerally accepted as reliable by polygraph experts; all polygraph materials must be provided to the\\nopposing party at least 10 days before trial; and all A3s conducted on the\\nsubject must be disclosed.',\n",
       "  \"it has been contended that the phrase `modern and\\nscientific techniques including dna profiling and such other tests' should be liberally construed to\\ninclude the impugned techniques. it was argued that even though the narcoanalysis technique,\\nA3 and the beap test have not been expressly enumerated, they could be read\\nin by examining the legislative intent. emphasis was placed on the phrase `and such other tests' to\\nargue that the parliament had chosen an approach where the list of `modern and scientific\\ntechniques' contemplated was illustrative and not exhaustive.\",\n",
       "  'when a person undergoes a narcoanalysis test, he/she is in a\\nhalf- conscious state and subsequently does not remember the revelations made in a drug-induced\\nstate. in the case of A3 and the beap test, the test subject remains fully\\nconscious during the tests but does not immediately know the nature and implications of the results\\nderived from the same. however, when he/she later learns about the contents of the revelations,\\nthey may prove to be incriminatory or be in the nature of testimony that can be used to prosecute\\nother individuals.',\n",
       "  \"hence, our conclusion is\\nthat the results obtained through the involuntary administration of either of the impugned tests (i.e. the narcoanalysis technique, A3 and the beap test) come within the scope of\\n`testimonial compulsion', thereby attracting the protective shield of article 20(3). ii.\",\n",
       "  \"160. even though the actual process of undergoing a A3 or a beap test is not\\nthe same as that of making an oral or written statement, the consequences are similar. by making\\ninferences from the results of these tests, the examiner is able to derive knowledge from the subject's\\nmind which otherwise would not have become available to the investigators.\",\n",
       "  'in that case, an eight judge majority decided\\nthat military rule of evidence 707 (which made polygraph results inadmissible in court-martial\\nproceedings) did not violate an accused person\\'s sixth amendment right to present a defence. the\\nrelevant part of the provision follows:\\n\"(a) notwithstanding any other provision of law, the results of a polygraph\\nexamination, the opinion of a polygraph examiner, or any reference to an offer to\\ntake, failure to take, or taking of a A3 shall not be admitted into\\nevidence.\" 31.',\n",
       "  \"the test needs to be conducted in an insulated\\nand air-conditioned room in order to prevent distortions arising out of weather conditions. much\\nlike the narcoanalysis technique and A3 this test also requires effective\\ncollaboration between the investigators and the examiner, most importantly for designing the\\nstimuli which are called `probes'. ascertaining the subject's familiarity with the `probes' can help in\\ndetecting deception or to gather useful information.\",\n",
       "  \"165. in light of the preceding discussion, we are of the view that the results obtained from tests such\\nas A3 and the beap test should also be treated as `personal testimony', since\\nthey are a means for `imparting personal knowledge about relevant facts'. hence, our conclusion is\\nthat the results obtained through the involuntary administration of either of the impugned tests (i.e.\",\n",
       "  \"furthermore, empirical studies have shown that during the hypnotic stage, individuals are prone to\\nsuggestibility and there is a good chance that false results could lead to a finding of guilt or\\ninnocence. as far as A3 is concerned, though there are some studies showing\\nimprovements in the accuracy of results with advancement in technology, there is always scope for\\nerror on account of several factors. objections can be raised about the qualifications of the\\nexaminer, the physical conditions under which the test was conducted, the manner in which\\nquestions were framed and the possible use of `countermeasures' by the test subject.\",\n",
       "  \"the theory behind polygraph tests is that when a subject is lying in response to a question,\\nhe/she will produce physiological responses that are different from those that arise in the normal\\ncourse. during the A3 several instruments are attached to the subject for\\nmeasuring and recording the physiological responses. the examiner then reads these results,\\nanalyzes them and proceeds to gauge the credibility of the subject's answers.\",\n",
       "  'indicates that this is an acceptable use. in this regard, para 3.4 (v) of the said manual reads as\\nfollows:\\n\"(v) in cases of alleged sex offences such as intercourse with a female child, forcible\\nrape, indecent liberties or perversion, it is important that the victim, as well as theselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 5accused, be made available for interview and A3 it is essential\\nthat the polygraph examiner get a first hand detailed statement from the victim, and\\nthe interview of the victim precede that of the suspect or witnesses.',\n",
       "  'it is essential\\nthat the polygraph examiner get a first hand detailed statement from the victim, and\\nthe interview of the victim precede that of the suspect or witnesses. ...\"\\n[the following article includes a table which lists out the statutorily permissible uses of polygraph\\nexamination in the different state jurisdictions of the united states of america:\\nhenry t. greely and judy illes, `neuroscience based lie-\\ndetection: the urgent need for regulation\\', 33 american journal of law and\\nmedicine, 377-421 (2007)]\\n15. the propriety of compelling the victims of sexual offences to undergo a A3\\ncertainly merits consideration in the present case.',\n",
       "  \"in both these tests, inferences are drawn\\nfrom the physiological responses of the subject and no direct reliance is placed on verbal responses. in some forms of A3 the subject may be required to offer verbal answers such\\nas `yes' or `no', but the results are based on the measurement of changes in several physiological\\ncharacteristics rather than these verbal responses. in the beap test, the subject is not required to\\ngive any verbal responses at all and inferences are drawn from the measurement of electrical activity\\nin the brain.\",\n",
       "  \"the difficulty arises since the majority opinion in\\nthat case appears to confine the understanding of `personal testimony' to the conveyance of\\npersonal knowledge through oral statements or statements in writing. the results obtained from\\nA3 or a beap test are not in the nature of oral or written statements. instead,\\ninferences are drawn from the measurement of physiological responses recorded during the\\nperformance of these tests.\",\n",
       "  'at that time, the transmission of knowledge\\nthrough means other than speech or writing was not something that could have been easily\\nconceived of. techniques such as A3 were fairly obscure and were the subject of\\nexperimentation in some western nations while the beap technique was developed several years\\nlater. just as the interpretation of statutes has to be often re-examined in light of scientific\\nadvancements, we should also be willing to re- examine judicial observations with a progressive\\nlens.',\n",
       "  \"in response, the counsel for the respondents have drawn our attention to literature which\\nsuggests that in the case of the impugned techniques, the intention on part of the investigators is to\\nextract information and not to inflict any pain or suffering. furthermore, it has been contended that\\nthe actual administration of either the narcoanalysis technique, A3 or the beap\\ntest does not involve a condemnable degree of `physical pain or suffering'. even though some\\nphysical force may be used or threats may be given to compel a person to undergo the tests, it was\\nargued that the administration of these tests ordinarily does not result in physical injuries.\",\n",
       "  '1995), the district court took note of new mexico rule of\\nevidence 11-707 which established standards for the admission of polygraph evidence. the said\\nprovision laid down that polygraph evidence would be admissible only when the following\\nconditions are met: the examiner must have had at least 5 years experience in conducting polygraph\\ntests and 20 hours of continuing education within the past year; the A3 must be\\ntape recorded in its entirety; the polygraph charts must be scored quantitatively in a manner\\ngenerally accepted as reliable by polygraph experts; all polygraph materials must be provided to the\\nopposing party at least 10 days before trial; and all A3s conducted on the\\nsubject must be disclosed. it was found that all of these requirements had been complied with in the\\nfacts at hand.',\n",
       "  '1. the legal questions in this batch of criminal appeals relate to the involuntary administration ofselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 1certain scientific techniques, namely narcoanalysis, A3 and the brain electrical\\nactivation profile (beap) test for the purpose of improving A7 in criminal cases. this issue has received considerable attention since it involves tensions between the desirability of\\nefficient A6 and the preservation of individual liberties.',\n",
       "  'in\\nmost cases these tests are conducted conjunctively wherein the veracity of the information revealed\\nthrough narcoanalysis is subsequently tested through a A3 or the beap test. in\\nsome cases the investigators could first want to ascertain the capacity of the subject to deceive\\n(through A3) or his/her familiarity with the relevant facts (through beap test)\\nbefore conducting a A2 irrespective of the sequence in which these techniques\\nare administered, we have to decide on their permissibility in circumstances where any of these tests\\nare compulsorily administered, either independently or conjunctively.',\n",
       "  'during their trial, one of their accomplices had given testimony which directly implicated them. the\\nrespondents contested this testimony and after the completion of the evidentiary phase of the trial,\\nthey moved an application to re-open their defence while seeking permission for each of them to\\nundergo a A3 and produce the results in evidence. the trial judge denied this\\nmotion and the respondents were convicted.',\n",
       "  \"in case of the narcoanalysis technique, the subject speaks in a\\ndrug-induced state and is clearly not aware of his/her own responses at the time. in the context of\\nA3 and the beap tests, the subject cannot anticipate the contents of the\\n`relevant questions' that will be asked or the `probes' that will be shown. furthermore, the results\\nare derived from the measurement of physiological responses and hence the subject cannot exercise\\nan effective choice between remaining silent and imparting personal knowledge.\",\n",
       "  '[see: laboratory procedure\\nmanual - A3 (directorate of forensic science, ministry of home affairs,\\ngovernment of india, new delhi - 2005)]\\n11. there are three prominent A3 techniques:\\ni. the relevant-irrelevant (r-i) technique ii.'],\n",
       " 'nan, NULL': ['however, this uncertainty was laid to rest by an authoritative ruling of the u.s. supreme\\ncourt in united states v. scheffer, 523 us 303 (1998). in that case, an eight judge majority decided\\nthat military rule of evidence 707 (which made polygraph results inadmissible in court-martial\\nproceedings) did not violate an accused person\\'s sixth amendment right to present a defence. the\\nrelevant part of the provision follows:\\n\"(a) notwithstanding any other provision of law, the results of a polygraph\\nexamination, the opinion of a polygraph examiner, or any reference to an offer to\\ntake, failure to take, or taking of a A3 shall not be admitted into\\nevidence.\"',\n",
       "  \"instead,\\ninferences are drawn from the measurement of physiological responses recorded during the\\nperformance of these tests. it could also be argued that tests such as A3 and the\\nbeap test do not involve a `positive volitional act' on part of the test subject and hence their results\\nshould not be treated as testimony. however, this does not entail that the results of these two testsselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 69should be likened to physical evidence and thereby excluded from the protective scope of article\\n20(3).\",\n",
       "  'as per the laboratory procedure manuals, the impugned tests are being conducted at the\\ndirection of jurisdictional courts even without obtaining the consent of the intended test subjects. in\\nmost cases these tests are conducted conjunctively wherein the veracity of the information revealed\\nthrough narcoanalysis is subsequently tested through a A3 or the beap test. in\\nsome cases the investigators could first want to ascertain the capacity of the subject to deceive\\n(through A3) or his/her familiarity with the relevant facts (through beap test)\\nbefore conducting a A2',\n",
       "  '...\"\\n[the following article includes a table which lists out the statutorily permissible uses of polygraph\\nexamination in the different state jurisdictions of the united states of america:\\nhenry t. greely and judy illes, `neuroscience based lie-\\ndetection: the urgent need for regulation\\', 33 american journal of law and\\nmedicine, 377-421 (2007)]\\n15. the propriety of compelling the victims of sexual offences to undergo a A3\\ncertainly merits consideration in the present case. it must also be noted that in some jurisdictions\\npolygraph tests have been permitted for the purpose of screening public employees, both at the\\nstage of recruitment and at regular intervals during the service-period.',\n",
       "  'the propriety of compelling the victims of sexual offences to undergo a A3\\ncertainly merits consideration in the present case. it must also be noted that in some jurisdictions\\npolygraph tests have been permitted for the purpose of screening public employees, both at the\\nstage of recruitment and at regular intervals during the service-period. in the u.s.a., the widespread\\nacceptance of polygraph tests for checking the antecedents and monitoring the conduct of public\\nemployees has encouraged private employers to resort to the same.',\n",
       "  'in this regard, para 3.4 (v) of the said manual reads as\\nfollows:\\n\"(v) in cases of alleged sex offences such as intercourse with a female child, forcible\\nrape, indecent liberties or perversion, it is important that the victim, as well as theselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 5accused, be made available for interview and A3 it is essential\\nthat the polygraph examiner get a first hand detailed statement from the victim, and\\nthe interview of the victim precede that of the suspect or witnesses. ...\"\\n[the following article includes a table which lists out the statutorily permissible uses of polygraph\\nexamination in the different state jurisdictions of the united states of america:\\nhenry t. greely and judy illes, `neuroscience based lie-\\ndetection: the urgent need for regulation\\', 33 american journal of law and\\nmedicine, 377-421 (2007)]\\n15.',\n",
       "  '10. the theory behind polygraph tests is that when a subject is lying in response to a question,\\nhe/she will produce physiological responses that are different from those that arise in the normal\\ncourse. during the A3 several instruments are attached to the subject for\\nmeasuring and recording the physiological responses.',\n",
       "  '14. another controversial use of polygraph tests has been on victims of sexual offences for testing\\nthe veracity of their allegations. while several states in the u.s.a. have enacted provisions to\\nprohibit such use, the text of the laboratory procedure manual for A3 [supra.]',\n",
       "  'descriptions of tests - uses, limitations and\\nprecedents A3\\n9. the origins of A3 have been traced back to the efforts of lombroso, a\\ncriminologist who experimented with a machine that measured blood pressure and pulse to assess\\nthe honesty of persons suspected of criminal conduct. his device was called a hydrosphygmograph.',\n",
       "  \"hence, on an aggregate understanding of the materials produced before us we lean towards the view\\nthat the impugned tests, i.e. the narcoanalysis technique, A3 and the beap test\\nshould not be read into the provisions for `medical examination' under the code of criminal\\nprocedure, 1973.selvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 67153. however, it must be borne in mind that even though the impugned techniques have not been\\nexpressly enumerated in the crpc, there is no statutory prohibition against them either.\",\n",
       "  'in united states v.\\ngalbreth, 908 f. supp 877 (d.n.m. 1995), the district court took note of new mexico rule of\\nevidence 11-707 which established standards for the admission of polygraph evidence. the said\\nprovision laid down that polygraph evidence would be admissible only when the following\\nconditions are met: the examiner must have had at least 5 years experience in conducting polygraph\\ntests and 20 hours of continuing education within the past year; the A3 must be\\ntape recorded in its entirety; the polygraph charts must be scored quantitatively in a manner\\ngenerally accepted as reliable by polygraph experts; all polygraph materials must be provided to the\\nopposing party at least 10 days before trial; and all A3s conducted on the\\nsubject must be disclosed.',\n",
       "  \"it has been contended that the phrase `modern and\\nscientific techniques including dna profiling and such other tests' should be liberally construed to\\ninclude the impugned techniques. it was argued that even though the narcoanalysis technique,\\nA3 and the beap test have not been expressly enumerated, they could be read\\nin by examining the legislative intent. emphasis was placed on the phrase `and such other tests' to\\nargue that the parliament had chosen an approach where the list of `modern and scientific\\ntechniques' contemplated was illustrative and not exhaustive.\",\n",
       "  'when a person undergoes a narcoanalysis test, he/she is in a\\nhalf- conscious state and subsequently does not remember the revelations made in a drug-induced\\nstate. in the case of A3 and the beap test, the test subject remains fully\\nconscious during the tests but does not immediately know the nature and implications of the results\\nderived from the same. however, when he/she later learns about the contents of the revelations,\\nthey may prove to be incriminatory or be in the nature of testimony that can be used to prosecute\\nother individuals.',\n",
       "  \"hence, our conclusion is\\nthat the results obtained through the involuntary administration of either of the impugned tests (i.e. the narcoanalysis technique, A3 and the beap test) come within the scope of\\n`testimonial compulsion', thereby attracting the protective shield of article 20(3). ii.\",\n",
       "  \"160. even though the actual process of undergoing a A3 or a beap test is not\\nthe same as that of making an oral or written statement, the consequences are similar. by making\\ninferences from the results of these tests, the examiner is able to derive knowledge from the subject's\\nmind which otherwise would not have become available to the investigators.\",\n",
       "  'in that case, an eight judge majority decided\\nthat military rule of evidence 707 (which made polygraph results inadmissible in court-martial\\nproceedings) did not violate an accused person\\'s sixth amendment right to present a defence. the\\nrelevant part of the provision follows:\\n\"(a) notwithstanding any other provision of law, the results of a polygraph\\nexamination, the opinion of a polygraph examiner, or any reference to an offer to\\ntake, failure to take, or taking of a A3 shall not be admitted into\\nevidence.\" 31.',\n",
       "  \"the test needs to be conducted in an insulated\\nand air-conditioned room in order to prevent distortions arising out of weather conditions. much\\nlike the narcoanalysis technique and A3 this test also requires effective\\ncollaboration between the investigators and the examiner, most importantly for designing the\\nstimuli which are called `probes'. ascertaining the subject's familiarity with the `probes' can help in\\ndetecting deception or to gather useful information.\",\n",
       "  '36 american criminal law review 87-116 (winter 1999) at\\np. 91]. needless to say, the polygraph examiner should be familiar with the details of\\nthe ongoing A6 to meet this end the investigators are required to share\\ncopies of documents such as the first information report (fir), medico-legal\\nreports (mlr) and post-mortem reports (pmr) depending on the nature of the\\nfacts being investigated.',\n",
       "  'the district court\\nhad refused to consider polygraph evidence given by the defendants in support of their version of\\nevents leading up to the E6 and their arrest. on appeal, the fifth circuit court held\\nthat the rationale for disregarding polygraph evidence did not survive the daubert decision. the\\ncourt proceeded to remand the case to the trial court and directed that the admissibility of the\\npolygraph results should be assessed as per the factors enumerated in daubert (supra.).',\n",
       "  \"165. in light of the preceding discussion, we are of the view that the results obtained from tests such\\nas A3 and the beap test should also be treated as `personal testimony', since\\nthey are a means for `imparting personal knowledge about relevant facts'. hence, our conclusion is\\nthat the results obtained through the involuntary administration of either of the impugned tests (i.e.\",\n",
       "  '1995), the facts related to a\\npre-trial evidentiary hearing where the defendants had asked for the exclusion of forty-four\\nkilograms of cocaine that had been recovered from their luggage at an airport. the district court\\nhad refused to consider polygraph evidence given by the defendants in support of their version of\\nevents leading up to the E6 and their arrest. on appeal, the fifth circuit court held\\nthat the rationale for disregarding polygraph evidence did not survive the daubert decision.',\n",
       "  \"the theory behind polygraph tests is that when a subject is lying in response to a question,\\nhe/she will produce physiological responses that are different from those that arise in the normal\\ncourse. during the A3 several instruments are attached to the subject for\\nmeasuring and recording the physiological responses. the examiner then reads these results,\\nanalyzes them and proceeds to gauge the credibility of the subject's answers.\",\n",
       "  \"furthermore, empirical studies have shown that during the hypnotic stage, individuals are prone to\\nsuggestibility and there is a good chance that false results could lead to a finding of guilt or\\ninnocence. as far as A3 is concerned, though there are some studies showing\\nimprovements in the accuracy of results with advancement in technology, there is always scope for\\nerror on account of several factors. objections can be raised about the qualifications of the\\nexaminer, the physical conditions under which the test was conducted, the manner in which\\nquestions were framed and the possible use of `countermeasures' by the test subject.\",\n",
       "  'indicates that this is an acceptable use. in this regard, para 3.4 (v) of the said manual reads as\\nfollows:\\n\"(v) in cases of alleged sex offences such as intercourse with a female child, forcible\\nrape, indecent liberties or perversion, it is important that the victim, as well as theselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 5accused, be made available for interview and A3 it is essential\\nthat the polygraph examiner get a first hand detailed statement from the victim, and\\nthe interview of the victim precede that of the suspect or witnesses.',\n",
       "  'it is essential\\nthat the polygraph examiner get a first hand detailed statement from the victim, and\\nthe interview of the victim precede that of the suspect or witnesses. ...\"\\n[the following article includes a table which lists out the statutorily permissible uses of polygraph\\nexamination in the different state jurisdictions of the united states of america:\\nhenry t. greely and judy illes, `neuroscience based lie-\\ndetection: the urgent need for regulation\\', 33 american journal of law and\\nmedicine, 377-421 (2007)]\\n15. the propriety of compelling the victims of sexual offences to undergo a A3\\ncertainly merits consideration in the present case.',\n",
       "  \"in both these tests, inferences are drawn\\nfrom the physiological responses of the subject and no direct reliance is placed on verbal responses. in some forms of A3 the subject may be required to offer verbal answers such\\nas `yes' or `no', but the results are based on the measurement of changes in several physiological\\ncharacteristics rather than these verbal responses. in the beap test, the subject is not required to\\ngive any verbal responses at all and inferences are drawn from the measurement of electrical activity\\nin the brain.\",\n",
       "  \"the difficulty arises since the majority opinion in\\nthat case appears to confine the understanding of `personal testimony' to the conveyance of\\npersonal knowledge through oral statements or statements in writing. the results obtained from\\nA3 or a beap test are not in the nature of oral or written statements. instead,\\ninferences are drawn from the measurement of physiological responses recorded during the\\nperformance of these tests.\",\n",
       "  'at that time, the transmission of knowledge\\nthrough means other than speech or writing was not something that could have been easily\\nconceived of. techniques such as A3 were fairly obscure and were the subject of\\nexperimentation in some western nations while the beap technique was developed several years\\nlater. just as the interpretation of statutes has to be often re-examined in light of scientific\\nadvancements, we should also be willing to re- examine judicial observations with a progressive\\nlens.',\n",
       "  \"in response, the counsel for the respondents have drawn our attention to literature which\\nsuggests that in the case of the impugned techniques, the intention on part of the investigators is to\\nextract information and not to inflict any pain or suffering. furthermore, it has been contended that\\nthe actual administration of either the narcoanalysis technique, A3 or the beap\\ntest does not involve a condemnable degree of `physical pain or suffering'. even though some\\nphysical force may be used or threats may be given to compel a person to undergo the tests, it was\\nargued that the administration of these tests ordinarily does not result in physical injuries.\",\n",
       "  '62. notably, the appellant had refused to undergo a A2 or a polygraph test. it\\nwas also evident that he had not consented to the hypnosis.',\n",
       "  '1995), the district court took note of new mexico rule of\\nevidence 11-707 which established standards for the admission of polygraph evidence. the said\\nprovision laid down that polygraph evidence would be admissible only when the following\\nconditions are met: the examiner must have had at least 5 years experience in conducting polygraph\\ntests and 20 hours of continuing education within the past year; the A3 must be\\ntape recorded in its entirety; the polygraph charts must be scored quantitatively in a manner\\ngenerally accepted as reliable by polygraph experts; all polygraph materials must be provided to the\\nopposing party at least 10 days before trial; and all A3s conducted on the\\nsubject must be disclosed. it was found that all of these requirements had been complied with in the\\nfacts at hand.',\n",
       "  '1. the legal questions in this batch of criminal appeals relate to the involuntary administration ofselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 1certain scientific techniques, namely narcoanalysis, A3 and the brain electrical\\nactivation profile (beap) test for the purpose of improving A7 in criminal cases. this issue has received considerable attention since it involves tensions between the desirability of\\nefficient A6 and the preservation of individual liberties.',\n",
       "  'in\\nmost cases these tests are conducted conjunctively wherein the veracity of the information revealed\\nthrough narcoanalysis is subsequently tested through a A3 or the beap test. in\\nsome cases the investigators could first want to ascertain the capacity of the subject to deceive\\n(through A3) or his/her familiarity with the relevant facts (through beap test)\\nbefore conducting a A2 irrespective of the sequence in which these techniques\\nare administered, we have to decide on their permissibility in circumstances where any of these tests\\nare compulsorily administered, either independently or conjunctively.',\n",
       "  'during their trial, one of their accomplices had given testimony which directly implicated them. the\\nrespondents contested this testimony and after the completion of the evidentiary phase of the trial,\\nthey moved an application to re-open their defence while seeking permission for each of them to\\nundergo a A3 and produce the results in evidence. the trial judge denied this\\nmotion and the respondents were convicted.',\n",
       "  \"in case of the narcoanalysis technique, the subject speaks in a\\ndrug-induced state and is clearly not aware of his/her own responses at the time. in the context of\\nA3 and the beap tests, the subject cannot anticipate the contents of the\\n`relevant questions' that will be asked or the `probes' that will be shown. furthermore, the results\\nare derived from the measurement of physiological responses and hence the subject cannot exercise\\nan effective choice between remaining silent and imparting personal knowledge.\",\n",
       "  '[see: laboratory procedure\\nmanual - A3 (directorate of forensic science, ministry of home affairs,\\ngovernment of india, new delhi - 2005)]\\n11. there are three prominent A3 techniques:\\ni. the relevant-irrelevant (r-i) technique ii.'],\n",
       " 'investigation efforts': ['1. the legal questions in this batch of criminal appeals relate to the involuntary administration ofselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 1certain scientific techniques, namely narcoanalysis, A3 and the brain electrical\\nactivation profile (beap) test for the purpose of improving A7 in criminal cases. this issue has received considerable attention since it involves tensions between the desirability of\\nefficient A6 and the preservation of individual liberties.'],\n",
       " 'investigation': ['1. the legal questions in this batch of criminal appeals relate to the involuntary administration ofselvi & ors vs state of karnataka & anr on 5 may, 2010\\nindian kanoon - http://indiankanoon.org/doc/338008/ 1certain scientific techniques, namely narcoanalysis, A3 and the brain electrical\\nactivation profile (beap) test for the purpose of improving A7 in criminal cases. this issue has received considerable attention since it involves tensions between the desirability of\\nefficient A6 and the preservation of individual liberties.',\n",
       "  '36 american criminal law review 87-116 (winter 1999) at\\np. 91]. needless to say, the polygraph examiner should be familiar with the details of\\nthe ongoing A6 to meet this end the investigators are required to share\\ncopies of documents such as the first information report (fir), medico-legal\\nreports (mlr) and post-mortem reports (pmr) depending on the nature of the\\nfacts being investigated.'],\n",
       " 'seizure of the drugs': ['1995), the facts related to a\\npre-trial evidentiary hearing where the defendants had asked for the exclusion of forty-four\\nkilograms of cocaine that had been recovered from their luggage at an airport. the district court\\nhad refused to consider polygraph evidence given by the defendants in support of their version of\\nevents leading up to the E6 and their arrest. on appeal, the fifth circuit court held\\nthat the rationale for disregarding polygraph evidence did not survive the daubert decision.',\n",
       "  'the district court\\nhad refused to consider polygraph evidence given by the defendants in support of their version of\\nevents leading up to the E6 and their arrest. on appeal, the fifth circuit court held\\nthat the rationale for disregarding polygraph evidence did not survive the daubert decision. the\\ncourt proceeded to remand the case to the trial court and directed that the admissibility of the\\npolygraph results should be assessed as per the factors enumerated in daubert (supra.).'],\n",
       " 'narcoanalysis interview': ['in\\nmost cases these tests are conducted conjunctively wherein the veracity of the information revealed\\nthrough narcoanalysis is subsequently tested through a A3 or the beap test. in\\nsome cases the investigators could first want to ascertain the capacity of the subject to deceive\\n(through A3) or his/her familiarity with the relevant facts (through beap test)\\nbefore conducting a A2 irrespective of the sequence in which these techniques\\nare administered, we have to decide on their permissibility in circumstances where any of these tests\\nare compulsorily administered, either independently or conjunctively.',\n",
       "  '62. notably, the appellant had refused to undergo a A2 or a polygraph test. it\\nwas also evident that he had not consented to the hypnosis.',\n",
       "  'as per the laboratory procedure manuals, the impugned tests are being conducted at the\\ndirection of jurisdictional courts even without obtaining the consent of the intended test subjects. in\\nmost cases these tests are conducted conjunctively wherein the veracity of the information revealed\\nthrough narcoanalysis is subsequently tested through a A3 or the beap test. in\\nsome cases the investigators could first want to ascertain the capacity of the subject to deceive\\n(through A3) or his/her familiarity with the relevant facts (through beap test)\\nbefore conducting a A2']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(str(sentences_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
