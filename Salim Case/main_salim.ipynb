{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_txt(pdf_file_path, txt_file_path):\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text().rstrip()\n",
    "\n",
    "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_to_txt('/Users/Desktop/ACM CONFERENCE WORK/FINAL_LIST_DOCS/Salman_Salim_Khan_vs_The_State_Of_Maharashtra_on_10_December_2015.PDF', 'salim.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the required NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_sentences(sentences, keyword):\n",
    "    related_sentences = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if keyword in sentence:\n",
    "            prev_sentence = sentences[i - 1].strip() if i > 0 else None\n",
    "            next_sentence = sentences[i + 1].strip() if i < len(sentences) - 1 else None\n",
    "            related_sentences.append({\n",
    "                \"Sentence\": sentence.strip(),\n",
    "                \"Previous\": prev_sentence,\n",
    "                \"Next\": next_sentence\n",
    "            })\n",
    "\n",
    "    return related_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Desktop/ACM CONFERENCE WORK/codes/Salim Case/salim.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize into sentences using nltk.sent_tokenize()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "keyword = \"witness\"\n",
    "result = []\n",
    "\n",
    "related_sentences = find_related_sentences(sentences, keyword)\n",
    "result.extend(related_sentences)\n",
    "\n",
    "# Create a DataFrame with columns \"Sentence,\" \"Previous,\" and \"Next\"\n",
    "output_df = pd.DataFrame(result, columns=[\"Sentence\", \"Previous\", \"Next\"])\n",
    "\n",
    "# Save results to a new CSV\n",
    "output_df.to_csv(\"output_sentences_salim.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read successfully with encoding: utf-8\n"
     ]
    }
   ],
   "source": [
    "encodings_to_try = ['utf-8', 'latin1', 'utf-16', 'ISO-8859-1']\n",
    "\n",
    "# Try reading the CSV file with different encodings\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        output_df = pd.read_csv(\"output_sentences_salim.csv\", encoding=encoding)\n",
    "        print(\"CSV file read successfully with encoding:\", encoding)\n",
    "        break  # Exit loop if successful\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Failed to read CSV file with encoding:\", encoding)\n",
    "        continue  # Try next encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Previous</th>\n",
       "      <th>Next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As 7 CR APEAL-572-2015-JUDGMENT.doc such, cons...</td>\n",
       "      <td>It is specifically mentioned that though the a...</td>\n",
       "      <td>Initially the matter was started for arguments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PW-1 one Sambha Gavda was one of the panch wit...</td>\n",
       "      <td>The spot\\npanchnama was conducted under the su...</td>\n",
       "      <td>Various articles were collected from\\nthe spot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During investigation, statements of various 19...</td>\n",
       "      <td>13.</td>\n",
       "      <td>The motor vehicle involved in the incident was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is the specific evidence of this witness ap...</td>\n",
       "      <td>According to him he inspected the vehicle on 2...</td>\n",
       "      <td>This is because of some answers\\ngiven by him ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>During investigation, statements of some of th...</td>\n",
       "      <td>16.</td>\n",
       "      <td>Statement of one Kamal Khan,\\na friend of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>In 289 CR APEAL-572-2015-JUDGMENT.doc fact, as...</td>\n",
       "      <td>Of course, this argument was strongly objected...</td>\n",
       "      <td>Otherwise, the witness can be asked questions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Otherwise, the witness can be asked questions ...</td>\n",
       "      <td>In 289 CR APEAL-572-2015-JUDGMENT.doc fact, as...</td>\n",
       "      <td>As such, considering this legal position and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>During the arguments, it is submitted that the...</td>\n",
       "      <td>Now, the last argument advanced on behalf of t...</td>\n",
       "      <td>It is an admitted position that crane was call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Even according to the witnesses and also the\\n...</td>\n",
       "      <td>On this aspect, it is\\nsubmitted on behalf of ...</td>\n",
       "      <td>As such considering this argument and the\\nfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>In this case, considering the various\\nweaknes...</td>\n",
       "      <td>The benefit of every reasonable doubt which ar...</td>\n",
       "      <td>On the basis of this type of evidence the appe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0    As 7 CR APEAL-572-2015-JUDGMENT.doc such, cons...   \n",
       "1    PW-1 one Sambha Gavda was one of the panch wit...   \n",
       "2    During investigation, statements of various 19...   \n",
       "3    It is the specific evidence of this witness ap...   \n",
       "4    During investigation, statements of some of th...   \n",
       "..                                                 ...   \n",
       "326  In 289 CR APEAL-572-2015-JUDGMENT.doc fact, as...   \n",
       "327  Otherwise, the witness can be asked questions ...   \n",
       "328  During the arguments, it is submitted that the...   \n",
       "329  Even according to the witnesses and also the\\n...   \n",
       "330  In this case, considering the various\\nweaknes...   \n",
       "\n",
       "                                              Previous  \\\n",
       "0    It is specifically mentioned that though the a...   \n",
       "1    The spot\\npanchnama was conducted under the su...   \n",
       "2                                                  13.   \n",
       "3    According to him he inspected the vehicle on 2...   \n",
       "4                                                  16.   \n",
       "..                                                 ...   \n",
       "326  Of course, this argument was strongly objected...   \n",
       "327  In 289 CR APEAL-572-2015-JUDGMENT.doc fact, as...   \n",
       "328  Now, the last argument advanced on behalf of t...   \n",
       "329  On this aspect, it is\\nsubmitted on behalf of ...   \n",
       "330  The benefit of every reasonable doubt which ar...   \n",
       "\n",
       "                                                  Next  \n",
       "0    Initially the matter was started for arguments...  \n",
       "1    Various articles were collected from\\nthe spot...  \n",
       "2    The motor vehicle involved in the incident was...  \n",
       "3    This is because of some answers\\ngiven by him ...  \n",
       "4    Statement of one Kamal Khan,\\na friend of the ...  \n",
       "..                                                 ...  \n",
       "326  Otherwise, the witness can be asked questions ...  \n",
       "327  As such, considering this legal position and c...  \n",
       "328  It is an admitted position that crane was call...  \n",
       "329  As such considering this argument and the\\nfac...  \n",
       "330  On the basis of this type of evidence the appe...  \n",
       "\n",
       "[331 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sen = pd.DataFrame(output_df['Previous'].astype(str) + ' ' + output_df['Sentence'].astype(str) + ' ' + output_df['Next'].astype(str), columns=['concat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is specifically mentioned that though the appeal is challenging the conviction for the main offence\n",
      "punishable under Section 304 Part II of IPC, various other aspects were also argued as to the\n",
      "involvement of the appellant as a driver of the motor vehicle involved in the incident and whether he\n",
      "was under the influence of alcohol or whether it was pure and simple accident due to bursting of the\n",
      "tyre of the vehicle. As 7 CR APEAL-572-2015-JUDGMENT.doc such, considering the scope of the\n",
      "matter and considering the conviction of the appellant awarded by the Sessions Court after\n",
      "examination of 27 witnesses, learned Senior Counsel for the appellant argued the matter since\n",
      "30.7.2015. Initially the matter was started for arguments on 30.7.2015 and was taken on 5.8.2015,\n",
      "6.8.2015 and 7.8.2015.\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_sen['concat'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_sen.to_csv('sentences_final_concatenated_salim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_person_id_map(file_path):\n",
    "    person_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            person_id_list = row[0]\n",
    "            person = person_id_list.strip()\n",
    "            person_id = row[1].strip()\n",
    "            person_id_map[person] = person_id\n",
    "\n",
    "    return person_id_map\n",
    "\n",
    "def create_location_id_map(file_path):\n",
    "    location_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            location_id_list = row[0]\n",
    "            location = location_id_list.strip()\n",
    "            location_id = row[1].strip()\n",
    "            location_id_map[location] = location_id\n",
    "\n",
    "    return location_id_map\n",
    "\n",
    "def create_time_id_map(file_path):\n",
    "    time_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            time_id_list = row[0].split(',')\n",
    "            time = time_id_list.strip()\n",
    "            time_id = row[1].strip()\n",
    "            time_id_map[time] = time_id\n",
    "\n",
    "    return time_id_map\n",
    "\n",
    "def create_event_id_map(file_path):\n",
    "    event_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            event_id_list = row[0].split(',')\n",
    "            event = event_id_list.strip()\n",
    "            event_id = row[1].strip()\n",
    "            event_id_map[event] = event_id\n",
    "\n",
    "    return event_id_map\n",
    "\n",
    "def create_other_id_map(file_path):\n",
    "    other_id_map = {}\n",
    "\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "        \n",
    "        for row in reader:\n",
    "            other_id_list = row[0].split(',')\n",
    "            other = other_id_list.strip()\n",
    "            other_id = row[1].strip()\n",
    "            other_id_map[other] = other_id\n",
    "\n",
    "    return other_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_person = '/Users/Desktop/ACM CONFERENCE WORK/Entities/Salim Case/person.csv'\n",
    "file_path_location = '/Users/Desktop/ACM CONFERENCE WORK/Entities/Salim Case/location.csv'\n",
    "file_path_time = '/Users/Desktop/ACM CONFERENCE WORK/Entities/Salim Case/time.csv'\n",
    "file_path_event = '/Users/Desktop/ACM CONFERENCE WORK/Entities/Salim Case/event.csv'\n",
    "file_path_other = '/Users/Desktop/ACM CONFERENCE WORK/Entities/Salim Case/activity.csv'\n",
    "person_id_map = create_person_id_map(file_path_person)\n",
    "location_id_map = create_location_id_map(file_path_location)\n",
    "time_id_map = create_location_id_map(file_path_time)\n",
    "event_id_map = create_location_id_map(file_path_event)\n",
    "other_id_map = create_location_id_map(file_path_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n=6):\n",
    "    words = text.split()\n",
    "    ngrams = []\n",
    "    for j in range(n, 0, -1):\n",
    "        for i in range(len(words)):\n",
    "            if i + j <= len(words):\n",
    "                ngrams.append(' '.join(words[i:i+j]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = text.replace(\".\", \"\")\n",
    "    text = text.replace(\";\", \"\")\n",
    "#     text = text.replace(\"-\", \"\")\n",
    "    text = text.replace(\"!\", \"\")\n",
    "    text = text.replace(\"?\", \"\")\n",
    "#     text = text.replace(\"/\", \"\")\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\",\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entities(text):\n",
    "    text = text.lower()\n",
    "    ngrams = generate_ngrams(text)\n",
    "    replaced_text = text\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        original = ngram.lower()\n",
    "        \n",
    "        if original in person_id_map:\n",
    "            entity_id = person_id_map[original]\n",
    "            replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "        else:\n",
    "            preprocessed_ngram = preprocess_text(ngram.lower())\n",
    "            if preprocessed_ngram in person_id_map:\n",
    "                entity_id = person_id_map[preprocessed_ngram]\n",
    "                replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "            else:\n",
    "                if original in location_id_map:\n",
    "                    entity_id = location_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in location_id_map:\n",
    "                    entity_id = location_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in time_id_map:\n",
    "                    entity_id = time_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in time_id_map:\n",
    "                    entity_id = time_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in event_id_map:\n",
    "                    entity_id = event_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in event_id_map:\n",
    "                    entity_id = event_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif original in other_id_map:\n",
    "                    entity_id = other_id_map[original]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                elif preprocessed_ngram in other_id_map:\n",
    "                    entity_id = other_id_map[preprocessed_ngram]\n",
    "                    replaced_text = replaced_text.replace(ngram, entity_id)\n",
    "                    \n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_final_concatenated_salim.csv', 'r', encoding='latin-1') as csvfile:\n",
    "     reader = csv.reader(csvfile)\n",
    "     next(reader)\n",
    "     ref_sen = []\n",
    "     for row in reader:\n",
    "         sentence = row[0]\n",
    "         ref_sen.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentences = []\n",
    "\n",
    "for sen in ref_sen:\n",
    "     s = replace_entities(sen)\n",
    "     final_sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_sentences_salim.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Sentences'])\n",
    "#      for sen in final_sentences:\n",
    "    for sen in final_sentences:\n",
    "        writer.writerow([sen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the sentences\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Load the CSV files\n",
    "time_df = pd.read_csv(file_path_time)\n",
    "location_df = pd.read_csv(file_path_location)\n",
    "event_df = pd.read_csv(file_path_event)\n",
    "person_df = pd.read_csv(file_path_person)\n",
    "activity_df = pd.read_csv(file_path_other)\n",
    "\n",
    "location_ID = location_df['Id'].tolist()\n",
    "location_df = location_df['Location'].tolist()\n",
    "\n",
    "time_ID = time_df['Id'].tolist()\n",
    "time_df = time_df['Time'].tolist()\n",
    "\n",
    "# event_ID = event_df['ID'].tolist()\n",
    "# event_df = event_df['Event'].tolist()\n",
    "\n",
    "# person_ID = person_df['ID'].tolist()\n",
    "# person_df = person_df['Person'].tolist()\n",
    "\n",
    "# activity_ID = activity_df['ID'].tolist()\n",
    "# activity_df = activity_df['Others'].tolist()\n",
    "\n",
    "event_ID_map = dict(zip(event_df['Id'], event_df['Event']))\n",
    "person_ID_map = dict(zip(person_df['Id'], person_df['Person']))\n",
    "activity_ID_map = dict(zip(activity_df['Id'], activity_df['Activity']))\n",
    "\n",
    "matrix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('cleaned_sentences_salim.csv', 'r', encoding='latin-1') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sentence = row[0]\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    pattern = r'([PTLEA]\\d+)'\n",
    "    matches = re.findall(pattern, s)\n",
    "\n",
    "    persons = []\n",
    "    time = []\n",
    "    location = []\n",
    "    event = []\n",
    "    activity = []\n",
    "\n",
    "    for match in matches:\n",
    "        if match[0] == 'P':\n",
    "            persons.append(person_ID_map.get(match, 'Unknown Person'))\n",
    "        elif match[0] == 'L':\n",
    "             location.append(location_ID.index(match))\n",
    "        elif match[0] == 'E':\n",
    "            event.append(event_ID_map.get(match, 'Unknown Event'))\n",
    "        elif match[0] == 'A':\n",
    "            activity.append(activity_ID_map.get(match, 'Unknown Activity'))\n",
    "        elif match[0] == 'T':\n",
    "            time.append(time_ID.index(match))\n",
    "\n",
    "    if location and time:\n",
    "        for loc_index in location:\n",
    "            loc_key = location_df[loc_index]\n",
    "            for time_index in time:\n",
    "                time_key = time_df[time_index]\n",
    "                if time_key not in matrix:\n",
    "                    matrix[time_key] = {}\n",
    "                if loc_key not in matrix[time_key]:\n",
    "                    matrix[time_key][loc_key] = []\n",
    "                matrix[time_key][loc_key].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    elif location:\n",
    "        for loc_index in location:\n",
    "            loc_key = location_df[loc_index]\n",
    "            if loc_key not in matrix:\n",
    "                matrix[loc_key] = {}\n",
    "            if 'NULL' not in matrix:\n",
    "                matrix['NULL'] = {}\n",
    "            if loc_key not in matrix[\"NULL\"]:\n",
    "                matrix[\"NULL\"][loc_key] = []\n",
    "            matrix[\"NULL\"][loc_key].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    elif time:\n",
    "        for time_index in time:\n",
    "            time_key = time_df[time_index]\n",
    "            if time_key not in matrix:\n",
    "                matrix[time_key] = {}\n",
    "            if 'NULL' not in matrix[time_key]:\n",
    "                matrix[time_key]['NULL'] = []\n",
    "            matrix[time_key][\"NULL\"].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "    else:\n",
    "        if 'NULL' not in matrix:\n",
    "            matrix['NULL'] = {}\n",
    "        if 'NULL' not in matrix['NULL']:\n",
    "            matrix['NULL']['NULL'] = []\n",
    "        matrix[\"NULL\"][\"NULL\"].extend([(entity, s) for entity in (persons + event + activity)])\n",
    "\n",
    "matrix_df = pd.DataFrame.from_dict(matrix, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df.to_csv('matrix_with_sen_salim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('matrix_with_sen_salim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Unnamed: 0': 'Time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time                         | NULL                                                                           |\n",
      "|------------------------------|--------------------------------------------------------------------------------|\n",
      "| 30.7.2015                    |                                                                                |\n",
      "| 6.8.2015                     |                                                                                |\n",
      "| nan                          | inspection report, an accident, ran away from the spot, ran away from the spot |\n",
      "| 29.9.2002 at about 9:30 a.m. |                                                                                |\n",
      "| 27.9.2002                    |                                                                                |\n",
      "| 31.1.2013                    |                                                                                |\n",
      "| 28.9.2002                    | an accident                                                                    |\n",
      "| at around 2:30 p.m.          |                                                                                |\n",
      "| 26.4.2014                    |                                                                                |\n",
      "| about 2:30 a.m. to 2:45 a.m. |                                                                                |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>NULL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.7.2015</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.8.2015</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>inspection report, an accident, ran away from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.9.2002 at about 9:30 a.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.9.2002</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.1.2013</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28.9.2002</td>\n",
       "      <td>an accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>at around 2:30 p.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26.4.2014</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>about 2:30 a.m. to 2:45 a.m.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Time  \\\n",
       "0                     30.7.2015   \n",
       "1                      6.8.2015   \n",
       "2                           NaN   \n",
       "3  29.9.2002 at about 9:30 a.m.   \n",
       "4                     27.9.2002   \n",
       "5                     31.1.2013   \n",
       "6                     28.9.2002   \n",
       "7           at around 2:30 p.m.   \n",
       "8                     26.4.2014   \n",
       "9  about 2:30 a.m. to 2:45 a.m.   \n",
       "\n",
       "                                                NULL  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2  inspection report, an accident, ran away from ...  \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6                                        an accident  \n",
       "7                                                     \n",
       "8                                                     \n",
       "9                                                     "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "def show_10x10_entity_matrix(df, max_entities=6):\n",
    "    # Take EXACT first 10 rows and first 10 columns\n",
    "    sub = df.iloc[:10, :10].copy()\n",
    "\n",
    "    # Convert each cell into entity-only listing\n",
    "    def extract_entities(cell):\n",
    "        if pd.isnull(cell) or cell == \"\":\n",
    "            return \"\"\n",
    "        try:\n",
    "            items = ast.literal_eval(cell)\n",
    "            ents = [e for e, _ in items]   # keep only entity IDs\n",
    "            if len(ents) > max_entities:\n",
    "                ents = ents[:max_entities] + [\"...\"]\n",
    "            return \", \".join(ents)\n",
    "        except:\n",
    "            return str(cell)\n",
    "\n",
    "    # Apply to all non-Time columns\n",
    "    for col in sub.columns:\n",
    "        if col != \"Time\":\n",
    "            sub[col] = sub[col].apply(extract_entities)\n",
    "\n",
    "    print(tabulate(sub, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "    return sub\n",
    "\n",
    "\n",
    "# RUN IT\n",
    "show_10x10_entity_matrix(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        # Convert the tuple to a string and add it as a node\n",
    "        G.add_node(f\"{time}, {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = set()\n",
    "\n",
    "# Update the final set with only the entities\n",
    "for value in pd.unique(df.drop(columns='Time').values.ravel()):\n",
    "    if pd.notnull(value):\n",
    "        value_list = ast.literal_eval(value)\n",
    "        entity_list = [entity for entity, sentence in value_list]\n",
    "        final.update(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        if pd.notnull(row[location]):\n",
    "            value_list = ast.literal_eval(row[location])\n",
    "            entity_list = [entity for entity, sentence in value_list]\n",
    "            for entity in entity_list:\n",
    "                if entity in final:\n",
    "                    # Convert the tuples to strings and add them as an edge\n",
    "                    G.add_edge(f\"{time}, {location}\", entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in list(G.nodes):\n",
    "    if not list(G.neighbors(node)):\n",
    "        G.remove_node(node)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node1 in G.nodes:\n",
    "    # Check if node1 can be split into a pair\n",
    "    if ', ' in node1:\n",
    "        parts1 = node1.split(\", \")\n",
    "        if len(parts1) == 2:  # Ensure it splits into exactly two parts\n",
    "            time1, location1 = parts1\n",
    "            for node2 in G.nodes:\n",
    "                # Check if node2 can be split into a pair\n",
    "                if ', ' in node2:\n",
    "                    parts2 = node2.split(\", \")\n",
    "                    if len(parts2) == 2:  # Ensure it splits into exactly two parts\n",
    "                        time2, location2 = parts2\n",
    "                        # Add edge if the time or location matches\n",
    "                        if time1 == time2 or location1 == location2:\n",
    "                            G.add_edge(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the sentences for each node\n",
    "sentences_dict = {}\n",
    "\n",
    "# Create a dictionary to store the sentences for each edge i.e. between two nodes\n",
    "edge_sentences_dict = {}\n",
    "\n",
    "# Update the sentences_dict and edge_sentences_dict\n",
    "for index, row in df.iterrows():\n",
    "    time = row['Time']\n",
    "    for location in df.columns:\n",
    "        if location == 'Time':\n",
    "            continue\n",
    "        if pd.notnull(row[location]):\n",
    "            value_list = ast.literal_eval(row[location])\n",
    "            for entity, sentence in value_list:\n",
    "                if entity in final:\n",
    "                    # Add the sentence to the sentences_dict for the entity\n",
    "                    if entity not in sentences_dict:\n",
    "                        sentences_dict[entity] = set()\n",
    "                    sentences_dict[entity].add(sentence)\n",
    "\n",
    "                    # Add the sentence to the sentences_dict for the (time, location) node\n",
    "                    time_location_node = f\"{time}, {location}\"\n",
    "                    if time_location_node not in sentences_dict:\n",
    "                        sentences_dict[time_location_node] = set()\n",
    "                    sentences_dict[time_location_node].add(sentence)\n",
    "\n",
    "                    # Add the sentence to the edge_sentences_dict for the edge\n",
    "                    edge = (time_location_node, entity)\n",
    "                    if edge not in edge_sentences_dict:\n",
    "                        edge_sentences_dict[edge] = set()\n",
    "                    edge_sentences_dict[edge].add(sentence)\n",
    "\n",
    "# Convert the sets back to lists if needed\n",
    "for node, sentences in sentences_dict.items():\n",
    "    sentences_dict[node] = list(sentences)\n",
    "\n",
    "for edge, sentences in edge_sentences_dict.items():\n",
    "    edge_sentences_dict[edge] = list(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: node2vec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (4.4.0)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (1.4.2)\n",
      "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (3.4.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from node2vec) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.15.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d603b26ad4445e8328f8fb0db57363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:00<00:00, 157.50it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:00<00:00, 132.73it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:00<00:00, 99.81it/s] \n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:00<00:00, 716.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "\n",
    "# Create a Node2Vec instance\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# Generate embeddings\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the node embeddings\n",
    "node_embeddings = {node: model.wv[node] for node in G.nodes}\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(list(node_embeddings.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Initialize DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2)  # Adjust parameters as needed\n",
    "\n",
    "# Fit and predict clusters\n",
    "labels = dbscan.fit_predict(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_colors = [\n",
    "    'rgb(31, 119, 180)',  # Blue\n",
    "    'rgb(255, 127, 14)',  # Orange\n",
    "    'rgb(44, 160, 44)',   # Green\n",
    "    'rgb(214, 39, 40)',   # Red\n",
    "    'rgb(148, 103, 189)', # Purple\n",
    "    'rgb(140, 86, 75)',   # Brown\n",
    "    'rgb(227, 119, 194)', # Pink\n",
    "    'rgb(127, 127, 127)', # Gray\n",
    "    'rgb(188, 189, 34)',  # Olive\n",
    "    'rgb(23, 190, 207)',  # Teal\n",
    "    'rgb(255, 187, 120)', # Peach\n",
    "    'rgb(214, 39, 40)',   # Maroon\n",
    "    'rgb(77, 175, 74)',   # Light Green\n",
    "    'rgb(152, 78, 163)',  # Plum\n",
    "    'rgb(255, 152, 150)'  # Salmon\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "none",
         "line": {
          "color": "gray",
          "width": 1
         },
         "mode": "lines",
         "type": "scatter",
         "x": [
          0.11384722041574198,
          0.7136012942083548,
          null,
          0.11384722041574198,
          -0.36216598639531344,
          null,
          0.11384722041574198,
          0.3839793337215856,
          null,
          0.11384722041574198,
          0.11384722041574198,
          null,
          0.11384722041574198,
          -0.34041875231747576,
          null,
          0.11384722041574198,
          -0.17705370775894386,
          null,
          -0.34041875231747576,
          -0.36216598639531344,
          null,
          -0.34041875231747576,
          -0.34041875231747576,
          null,
          -0.34041875231747576,
          -0.17705370775894386,
          null,
          -0.17705370775894386,
          -0.33178940187394956,
          null,
          -0.17705370775894386,
          -0.17705370775894386,
          null
         ],
         "y": [
          0.1703561043661886,
          0.09283426504324821,
          null,
          0.1703561043661886,
          0.43545652547405517,
          null,
          0.1703561043661886,
          0.7224265914085755,
          null,
          0.1703561043661886,
          0.1703561043661886,
          null,
          0.1703561043661886,
          0.008757114962950688,
          null,
          0.1703561043661886,
          -0.42983060125501804,
          null,
          0.008757114962950688,
          0.43545652547405517,
          null,
          0.008757114962950688,
          0.008757114962950688,
          null,
          0.008757114962950688,
          -0.42983060125501804,
          null,
          -0.42983060125501804,
          -1,
          null,
          -0.42983060125501804,
          -0.42983060125501804,
          null
         ]
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(31, 119, 180)",
          "line": {
           "color": "black",
           "width": 2
          },
          "showscale": false,
          "size": 20
         },
         "mode": "markers+text",
         "text": [
          "nan, NULL",
          "28.9.2002, NULL",
          "on 1.10.2002, NULL",
          "an accident",
          "analysed the blood phials.",
          "ran away from the spot",
          "inspection report"
         ],
         "textposition": "top center",
         "type": "scatter",
         "x": [
          0.11384722041574198,
          -0.34041875231747576,
          -0.17705370775894386,
          -0.36216598639531344,
          -0.33178940187394956,
          0.3839793337215856,
          0.7136012942083548
         ],
         "y": [
          0.1703561043661886,
          0.008757114962950688,
          -0.42983060125501804,
          0.43545652547405517,
          -1,
          0.7224265914085755,
          0.09283426504324821
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hovermode": "closest",
        "margin": {
         "b": 20,
         "l": 5,
         "r": 5,
         "t": 40
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Cluster 1"
        },
        "width": 600,
        "xaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        },
        "yaxis": {
         "showgrid": false,
         "showticklabels": false,
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Assuming 'G' is your graph and 'labels' is the list of labels\n",
    "# Renumber clusters from 1 to 7 if needed, otherwise ensure you have clusters numbered 1 to 7\n",
    "unique_labels = list(set(labels))\n",
    "label_mapping = {label: idx+1 for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "def draw_subgraph_plotly(subgraph, cluster_id, cluster_color):\n",
    "    pos = nx.spring_layout(subgraph, seed=42)  # Fixed seed for reproducibility\n",
    "    \n",
    "    # Extract the node positions\n",
    "    x_nodes = [pos[node][0] for node in subgraph.nodes()]\n",
    "    y_nodes = [pos[node][1] for node in subgraph.nodes()]\n",
    "\n",
    "    # Extract the edges\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in subgraph.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    # Edge trace\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1, color='gray'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines')\n",
    "\n",
    "    # Node trace\n",
    "    node_trace = go.Scatter(\n",
    "        x=x_nodes, y=y_nodes,\n",
    "        mode='markers+text',\n",
    "        text=[f'{node}' for node in subgraph.nodes()],\n",
    "        textposition=\"top center\",\n",
    "        marker=dict(\n",
    "            showscale=False,\n",
    "            color=cluster_color,  # Assign the cluster color\n",
    "            size=20,\n",
    "            line=dict(width=2, color='black')\n",
    "        ),\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=f'Cluster {cluster_id}',\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20, l=5, r=5, t=40),\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        height=600,\n",
    "                        width=600,\n",
    "                        paper_bgcolor='white',\n",
    "                        plot_bgcolor='white'\n",
    "                    ))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Iterate over each cluster and draw the subgraph\n",
    "for original_cluster_id in set(labels):\n",
    "    cluster_id = label_mapping[original_cluster_id]\n",
    "    cluster_color = cluster_colors[cluster_id - 1]  # Assign a unique color to each cluster\n",
    "    cluster_nodes = [node for node, label in zip(G.nodes, labels) if label == original_cluster_id]\n",
    "    subgraph = G.subgraph(cluster_nodes)\n",
    "    draw_subgraph_plotly(subgraph, cluster_id, cluster_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists for nodes and links\n",
    "# nodes = []\n",
    "# links = []\n",
    "\n",
    "# # Add nodes to the list\n",
    "# for node, sentences in sentences_dict.items():\n",
    "#     nodes.append({\n",
    "#         \"id\": node,\n",
    "#         \"group\": 1,  # Update this as needed\n",
    "#         \"size\": len(sentences)  # The size could be based on the number of sentences\n",
    "#     })\n",
    "\n",
    "# # Add links to the list\n",
    "# for edge, sentences in edge_sentences_dict.items():\n",
    "#     source, target = edge\n",
    "#     links.append({\n",
    "#         \"source\": source,\n",
    "#         \"target\": target,\n",
    "#         \"value\": len(sentences)  # The value could be based on the number of sentences\n",
    "#     })\n",
    "\n",
    "# # Combine nodes and links into a single dictionary\n",
    "# graph_data = {\n",
    "#     \"nodes\": nodes,\n",
    "#     \"links\": links\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['METIS_DLL'] = '/Users/.local/lib/libmetis.dylib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edgecuts, parts = metis.part_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create a mapping from nodes to integers\n",
    "node_to_int = {node: i for i, node in enumerate(G.nodes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/networkx/readwrite/json_graph/node_link.py:142: FutureWarning:\n",
      "\n",
      "\n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')\n",
    "\n",
    "data = nx.node_link_data(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = {\n",
    "    \"nodes\": [{\"name\": str(node), \"n\": degree_dict[node], \"grp\": int(labels[i]), \"id\": str(node)} for i, node in enumerate(G.nodes())],\n",
    "    \"links\": [{\"source\": str(link_data['source']), \"target\": str(link_data['target']), \"value\": 1} for link_data in data['links']]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(graph_data)\n",
    "with open('data_salim.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# # Create a dictionary where keys are part numbers and values are lists of nodes\n",
    "# part_dict = defaultdict(list)\n",
    "# for node, part in enumerate(parts):\n",
    "#     part_dict[part].append(node)\n",
    "\n",
    "# # Print the number of parts\n",
    "# print(\"Number of parts:\", len(part_dict))\n",
    "\n",
    "# # Print the nodes in each part\n",
    "# for part, nodes in part_dict.items():\n",
    "#     print(\"Part\", part, \":\", nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already created clusters using DBSCAN and have 'labels' and 'G' available\n",
    "\n",
    "# Initialize an empty dictionary to store sentences for each cluster\n",
    "cluster_sentences_dict = {}\n",
    "\n",
    "# Iterate over each cluster\n",
    "for original_cluster_id in set(labels):\n",
    "    cluster_id = label_mapping[original_cluster_id]\n",
    "    cluster_nodes = [node for node, label in zip(G.nodes, labels) if label == original_cluster_id]\n",
    "    \n",
    "    # Initialize a set to store sentences for this cluster\n",
    "    cluster_sentences = set()\n",
    "    \n",
    "    # Iterate over nodes in the cluster\n",
    "    for node in cluster_nodes:\n",
    "        # Assuming 'node' contains the relevant information (e.g., entity, time, location)\n",
    "        # Extract sentences associated with this node and add them to the cluster_sentences set\n",
    "        if node in sentences_dict:\n",
    "            cluster_sentences.update(sentences_dict[node])\n",
    "    \n",
    "    # Store the cluster_sentences set in the cluster_sentences_dict\n",
    "    cluster_sentences_dict[cluster_id] = cluster_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inspection report': ['the car involved in the\\nincident and gave A11 (exh.84). this witness stated that the tyres of the\\nvehicle were found in good condition and only stated that the left side front tyre was\\ndeflated. pw-1 is the panch witness sambha gowda regarding spot of incident.'],\n",
       " 'nan, NULL': ['in any event, this substantive evidence of\\npw 4 may not be of much significance so far as the case of the prosecution is concerned as to driving\\nand that also under the influence of alcohol by the appellant. however, during the\\ncross-examination still another admission is taken from this witness and this witness in paragraph 8\\nin his notes of evidence states to the following effect:\\n71 cr apeal-572-2015-judgment.doc \"it did not happen that two persons ran away from the\\ncar. the second person E15 after salman.',\n",
       "  'according to him, many people were telling that accused got down from the car. also, according\\nto this witness, accused, then E15 after seeing the crowd. one police guard was\\nalso present in the car.',\n",
       "  '53 cr apeal-572-2015-judgment.doc\\n44. now having the broad analysis the substantive evidence before the sessions court vis-a-\\nvis the arguments on behalf of the appellant and the state, a detailed analysis of the evidence and its\\nacceptability and the trustworthiness of the witnesses is required to be done on the broad three\\naspects as to who was driving, whether the appellant was drunk and whether it was E13 so\\nalso the other allied submissions are also required to be dealt in detail.',\n",
       "  'the car involved in the\\nincident and gave A11 (exh.84). this witness stated that the tyres of the\\nvehicle were found in good condition and only stated that the left side front tyre was\\ndeflated. pw-1 is the panch witness sambha gowda regarding spot of incident.'],\n",
       " 'an accident': ['he further stated that he did not\\nsee the case papers. now, a very different story is given by this witness during cross-examination\\nwhich is appearing in paragraph 12 of his notes of evidence to the following effect:\\n184 cr apeal-572-2015-judgment.doc \"it is true that i came to know that on the morning of\\nT11 that E13 had occurred. i also made a call to the control room on 29.9.2002 as to\\nwhether an inspection of the vehicle involved in the accident is to be carried or not.',\n",
       "  '53 cr apeal-572-2015-judgment.doc\\n44. now having the broad analysis the substantive evidence before the sessions court vis-a-\\nvis the arguments on behalf of the appellant and the state, a detailed analysis of the evidence and its\\nacceptability and the trustworthiness of the witnesses is required to be done on the broad three\\naspects as to who was driving, whether the appellant was drunk and whether it was E13 so\\nalso the other allied submissions are also required to be dealt in detail.'],\n",
       " 'ran away from the spot': ['in any event, this substantive evidence of\\npw 4 may not be of much significance so far as the case of the prosecution is concerned as to driving\\nand that also under the influence of alcohol by the appellant. however, during the\\ncross-examination still another admission is taken from this witness and this witness in paragraph 8\\nin his notes of evidence states to the following effect:\\n71 cr apeal-572-2015-judgment.doc \"it did not happen that two persons ran away from the\\ncar. the second person E15 after salman.',\n",
       "  'according to him, many people were telling that accused got down from the car. also, according\\nto this witness, accused, then E15 after seeing the crowd. one police guard was\\nalso present in the car.'],\n",
       " '28.9.2002, NULL': ['he further stated that he did not\\nsee the case papers. now, a very different story is given by this witness during cross-examination\\nwhich is appearing in paragraph 12 of his notes of evidence to the following effect:\\n184 cr apeal-572-2015-judgment.doc \"it is true that i came to know that on the morning of\\nT11 that E13 had occurred. i also made a call to the control room on 29.9.2002 as to\\nwhether an inspection of the vehicle involved in the accident is to be carried or not.'],\n",
       " 'analysed the blood phials.': ['thereafter, he kept the\\nblood phials in the refrigerator. further the evidence of this witness shows that T24 he\\nA10 he used the \"modified diffusion oxidation method\" for analyzing the\\nblood.'],\n",
       " 'on 1.10.2002, NULL': ['thereafter, he kept the\\nblood phials in the refrigerator. further the evidence of this witness shows that T24 he\\nA10 he used the \"modified diffusion oxidation method\" for analyzing the\\nblood.']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(str(sentences_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
